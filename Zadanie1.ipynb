{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8c8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d1fad",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399516fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0       1       2       3       4       5       6       7       8   \\\n",
      "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
      "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
      "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
      "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
      "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
      "\n",
      "       9   ...      51      52      53      54      55      56      57  \\\n",
      "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
      "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
      "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
      "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
      "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
      "\n",
      "       58      59  60  \n",
      "0  0.0090  0.0032   R  \n",
      "1  0.0052  0.0044   R  \n",
      "2  0.0095  0.0078   R  \n",
      "3  0.0040  0.0117   R  \n",
      "4  0.0107  0.0094   R  \n",
      "\n",
      "[5 rows x 61 columns]\n",
      "(208, 61)\n",
      "M    111\n",
      "R     97\n",
      "Name: 60, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sonar_data = pd.read_csv('dataset/sonar.all-data', header=None)\n",
    "print(sonar_data.head())\n",
    "print(sonar_data.shape)\n",
    "print(sonar_data[60].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1be166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "               6           7           8           9   ...          50  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \n",
       "std      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \n",
       "min      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n",
       "25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n",
       "50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n",
       "75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \n",
       "max      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n",
       "\n",
       "               51          52          53          54          55          56  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \n",
       "std      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \n",
       "min      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n",
       "25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n",
       "50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n",
       "75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \n",
       "max      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n",
       "\n",
       "               57          58          59  \n",
       "count  208.000000  208.000000  208.000000  \n",
       "mean     0.007949    0.007941    0.006507  \n",
       "std      0.006470    0.006181    0.005031  \n",
       "min      0.000300    0.000100    0.000600  \n",
       "25%      0.003600    0.003675    0.003100  \n",
       "50%      0.005800    0.006400    0.005300  \n",
       "75%      0.010350    0.010325    0.008525  \n",
       "max      0.044000    0.036400    0.043900  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fd4bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   0  \n",
       "1  0.0052  0.0044   0  \n",
       "2  0.0095  0.0078   0  \n",
       "3  0.0040  0.0117   0  \n",
       "4  0.0107  0.0094   0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacnutie R za 0 a M za 1\n",
    "# R - Rock M - Mina\n",
    "sonar_data[60] = sonar_data[60].replace(['R', 'M'], [0, 1])\n",
    "sonar_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90d065",
   "metadata": {},
   "source": [
    "Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43fefc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.136431</td>\n",
       "      <td>0.156451</td>\n",
       "      <td>0.135677</td>\n",
       "      <td>0.035426</td>\n",
       "      <td>0.224956</td>\n",
       "      <td>0.237571</td>\n",
       "      <td>0.407468</td>\n",
       "      <td>0.340904</td>\n",
       "      <td>0.449282</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027104</td>\n",
       "      <td>0.155844</td>\n",
       "      <td>0.435673</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.417949</td>\n",
       "      <td>0.502841</td>\n",
       "      <td>0.185355</td>\n",
       "      <td>0.245179</td>\n",
       "      <td>0.060046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.323009</td>\n",
       "      <td>0.221603</td>\n",
       "      <td>0.272011</td>\n",
       "      <td>0.150024</td>\n",
       "      <td>0.283033</td>\n",
       "      <td>0.666756</td>\n",
       "      <td>0.574405</td>\n",
       "      <td>0.755458</td>\n",
       "      <td>0.483045</td>\n",
       "      <td>0.394537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108417</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.199546</td>\n",
       "      <td>0.479487</td>\n",
       "      <td>0.389205</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.087760</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.182153</td>\n",
       "      <td>0.246892</td>\n",
       "      <td>0.356110</td>\n",
       "      <td>0.243699</td>\n",
       "      <td>0.230028</td>\n",
       "      <td>0.585327</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.819405</td>\n",
       "      <td>0.817859</td>\n",
       "      <td>0.869584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319544</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.248538</td>\n",
       "      <td>0.394558</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.889205</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.258953</td>\n",
       "      <td>0.166282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.062684</td>\n",
       "      <td>0.070724</td>\n",
       "      <td>0.199737</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>0.034999</td>\n",
       "      <td>0.071486</td>\n",
       "      <td>0.288149</td>\n",
       "      <td>0.269239</td>\n",
       "      <td>0.077447</td>\n",
       "      <td>0.164593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161198</td>\n",
       "      <td>0.080519</td>\n",
       "      <td>0.409357</td>\n",
       "      <td>0.179138</td>\n",
       "      <td>0.176923</td>\n",
       "      <td>0.133523</td>\n",
       "      <td>0.093822</td>\n",
       "      <td>0.107438</td>\n",
       "      <td>0.256351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.550885</td>\n",
       "      <td>0.282898</td>\n",
       "      <td>0.153088</td>\n",
       "      <td>0.079886</td>\n",
       "      <td>0.132640</td>\n",
       "      <td>0.147003</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.531863</td>\n",
       "      <td>0.516659</td>\n",
       "      <td>0.621479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032810</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.235828</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>0.196023</td>\n",
       "      <td>0.102975</td>\n",
       "      <td>0.292011</td>\n",
       "      <td>0.203233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.126844</td>\n",
       "      <td>0.145735</td>\n",
       "      <td>0.050263</td>\n",
       "      <td>0.028293</td>\n",
       "      <td>0.082678</td>\n",
       "      <td>0.410642</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>0.361411</td>\n",
       "      <td>0.333629</td>\n",
       "      <td>0.367653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154066</td>\n",
       "      <td>0.241558</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.248718</td>\n",
       "      <td>0.176136</td>\n",
       "      <td>0.256293</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.348730</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.227139</td>\n",
       "      <td>0.040720</td>\n",
       "      <td>0.092970</td>\n",
       "      <td>0.120304</td>\n",
       "      <td>0.175755</td>\n",
       "      <td>0.230046</td>\n",
       "      <td>0.258929</td>\n",
       "      <td>0.212348</td>\n",
       "      <td>0.141419</td>\n",
       "      <td>0.291863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075606</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.365497</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>0.151282</td>\n",
       "      <td>0.088068</td>\n",
       "      <td>0.066362</td>\n",
       "      <td>0.168044</td>\n",
       "      <td>0.140878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.373894</td>\n",
       "      <td>0.184741</td>\n",
       "      <td>0.054205</td>\n",
       "      <td>0.055635</td>\n",
       "      <td>0.072026</td>\n",
       "      <td>0.287288</td>\n",
       "      <td>0.331169</td>\n",
       "      <td>0.247630</td>\n",
       "      <td>0.175181</td>\n",
       "      <td>0.345488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216833</td>\n",
       "      <td>0.062338</td>\n",
       "      <td>0.119883</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>0.389205</td>\n",
       "      <td>0.308924</td>\n",
       "      <td>0.209366</td>\n",
       "      <td>0.057737</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.212389</td>\n",
       "      <td>0.148736</td>\n",
       "      <td>0.156045</td>\n",
       "      <td>0.130766</td>\n",
       "      <td>0.025361</td>\n",
       "      <td>0.336469</td>\n",
       "      <td>0.387446</td>\n",
       "      <td>0.235502</td>\n",
       "      <td>0.276914</td>\n",
       "      <td>0.320463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111270</td>\n",
       "      <td>0.106494</td>\n",
       "      <td>0.339181</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.079487</td>\n",
       "      <td>0.088068</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.096419</td>\n",
       "      <td>0.096998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.180678</td>\n",
       "      <td>0.153022</td>\n",
       "      <td>0.039750</td>\n",
       "      <td>0.050880</td>\n",
       "      <td>0.037281</td>\n",
       "      <td>0.063424</td>\n",
       "      <td>0.168290</td>\n",
       "      <td>0.296582</td>\n",
       "      <td>0.261810</td>\n",
       "      <td>0.320463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196862</td>\n",
       "      <td>0.322078</td>\n",
       "      <td>0.108187</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.146154</td>\n",
       "      <td>0.105114</td>\n",
       "      <td>0.075515</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.251732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.136431  0.156451  0.135677  0.035426  0.224956  0.237571  0.407468   \n",
       "1    0.323009  0.221603  0.272011  0.150024  0.283033  0.666756  0.574405   \n",
       "2    0.182153  0.246892  0.356110  0.243699  0.230028  0.585327  0.648810   \n",
       "3    0.062684  0.070724  0.199737  0.034950  0.034999  0.071486  0.288149   \n",
       "4    0.550885  0.282898  0.153088  0.079886  0.132640  0.147003  0.318182   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "203  0.126844  0.145735  0.050263  0.028293  0.082678  0.410642  0.539773   \n",
       "204  0.227139  0.040720  0.092970  0.120304  0.175755  0.230046  0.258929   \n",
       "205  0.373894  0.184741  0.054205  0.055635  0.072026  0.287288  0.331169   \n",
       "206  0.212389  0.148736  0.156045  0.130766  0.025361  0.336469  0.387446   \n",
       "207  0.180678  0.153022  0.039750  0.050880  0.037281  0.063424  0.168290   \n",
       "\n",
       "           7         8         9   ...        51        52        53  \\\n",
       "0    0.340904  0.449282  0.285714  ...  0.027104  0.155844  0.435673   \n",
       "1    0.755458  0.483045  0.394537  ...  0.108417  0.218182  0.111111   \n",
       "2    0.819405  0.817859  0.869584  ...  0.319544  0.418182  0.248538   \n",
       "3    0.269239  0.077447  0.164593  ...  0.161198  0.080519  0.409357   \n",
       "4    0.531863  0.516659  0.621479  ...  0.032810  0.127273  0.277778   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "203  0.361411  0.333629  0.367653  ...  0.154066  0.241558  0.552632   \n",
       "204  0.212348  0.141419  0.291863  ...  0.075606  0.228571  0.365497   \n",
       "205  0.247630  0.175181  0.345488  ...  0.216833  0.062338  0.119883   \n",
       "206  0.235502  0.276914  0.320463  ...  0.111270  0.106494  0.339181   \n",
       "207  0.296582  0.261810  0.320463  ...  0.196862  0.322078  0.108187   \n",
       "\n",
       "           54        55        56        57        58        59  60  \n",
       "0    0.149660  0.417949  0.502841  0.185355  0.245179  0.060046   0  \n",
       "1    0.199546  0.479487  0.389205  0.105263  0.140496  0.087760   0  \n",
       "2    0.394558  0.615385  0.889205  0.368421  0.258953  0.166282   0  \n",
       "3    0.179138  0.176923  0.133523  0.093822  0.107438  0.256351   0  \n",
       "4    0.235828  0.028205  0.196023  0.102975  0.292011  0.203233   0  \n",
       "..        ...       ...       ...       ...       ...       ...  ..  \n",
       "203  0.061224  0.248718  0.176136  0.256293  0.528926  0.348730   1  \n",
       "204  0.129252  0.151282  0.088068  0.066362  0.168044  0.140878   1  \n",
       "205  0.126984  0.217949  0.389205  0.308924  0.209366  0.057737   1  \n",
       "206  0.068027  0.079487  0.088068  0.173913  0.096419  0.096998   1  \n",
       "207  0.074830  0.146154  0.105114  0.075515  0.165289  0.251732   1  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df = sonar_data.copy()\n",
    "for x in range(60):\n",
    "    normalized_df[x] = MinMaxScaler().fit_transform(np.array(normalized_df[x]).reshape(-1,1))\n",
    "\n",
    "normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cb29279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.204011</td>\n",
       "      <td>0.162180</td>\n",
       "      <td>0.139068</td>\n",
       "      <td>0.114342</td>\n",
       "      <td>0.173732</td>\n",
       "      <td>0.253615</td>\n",
       "      <td>0.320472</td>\n",
       "      <td>0.285114</td>\n",
       "      <td>0.252485</td>\n",
       "      <td>0.281652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180031</td>\n",
       "      <td>0.265172</td>\n",
       "      <td>0.290669</td>\n",
       "      <td>0.197061</td>\n",
       "      <td>0.200555</td>\n",
       "      <td>0.213642</td>\n",
       "      <td>0.175035</td>\n",
       "      <td>0.216015</td>\n",
       "      <td>0.136425</td>\n",
       "      <td>0.533654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.169550</td>\n",
       "      <td>0.141277</td>\n",
       "      <td>0.126242</td>\n",
       "      <td>0.110623</td>\n",
       "      <td>0.140888</td>\n",
       "      <td>0.158843</td>\n",
       "      <td>0.167175</td>\n",
       "      <td>0.187767</td>\n",
       "      <td>0.175311</td>\n",
       "      <td>0.192215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137432</td>\n",
       "      <td>0.183385</td>\n",
       "      <td>0.213474</td>\n",
       "      <td>0.160717</td>\n",
       "      <td>0.147080</td>\n",
       "      <td>0.164361</td>\n",
       "      <td>0.148051</td>\n",
       "      <td>0.170286</td>\n",
       "      <td>0.116190</td>\n",
       "      <td>0.500070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.087389</td>\n",
       "      <td>0.067938</td>\n",
       "      <td>0.057326</td>\n",
       "      <td>0.044163</td>\n",
       "      <td>0.079508</td>\n",
       "      <td>0.152714</td>\n",
       "      <td>0.209957</td>\n",
       "      <td>0.165215</td>\n",
       "      <td>0.132571</td>\n",
       "      <td>0.142964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092368</td>\n",
       "      <td>0.118831</td>\n",
       "      <td>0.127924</td>\n",
       "      <td>0.080499</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.096591</td>\n",
       "      <td>0.075515</td>\n",
       "      <td>0.098485</td>\n",
       "      <td>0.057737</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.157080</td>\n",
       "      <td>0.129447</td>\n",
       "      <td>0.107753</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>0.141517</td>\n",
       "      <td>0.220236</td>\n",
       "      <td>0.280438</td>\n",
       "      <td>0.235061</td>\n",
       "      <td>0.214349</td>\n",
       "      <td>0.244673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151213</td>\n",
       "      <td>0.235065</td>\n",
       "      <td>0.242690</td>\n",
       "      <td>0.156463</td>\n",
       "      <td>0.165385</td>\n",
       "      <td>0.160511</td>\n",
       "      <td>0.125858</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.108545</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.251106</td>\n",
       "      <td>0.202958</td>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.139563</td>\n",
       "      <td>0.237319</td>\n",
       "      <td>0.333042</td>\n",
       "      <td>0.407738</td>\n",
       "      <td>0.361852</td>\n",
       "      <td>0.334555</td>\n",
       "      <td>0.368082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227175</td>\n",
       "      <td>0.374026</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.260897</td>\n",
       "      <td>0.287642</td>\n",
       "      <td>0.229977</td>\n",
       "      <td>0.281680</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.204011    0.162180    0.139068    0.114342    0.173732    0.253615   \n",
       "std      0.169550    0.141277    0.126242    0.110623    0.140888    0.158843   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.087389    0.067938    0.057326    0.044163    0.079508    0.152714   \n",
       "50%      0.157080    0.129447    0.107753    0.090942    0.141517    0.220236   \n",
       "75%      0.251106    0.202958    0.185447    0.139563    0.237319    0.333042   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               6           7           8           9   ...          51  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \n",
       "mean     0.320472    0.285114    0.252485    0.281652  ...    0.180031   \n",
       "std      0.167175    0.187767    0.175311    0.192215  ...    0.137432   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.209957    0.165215    0.132571    0.142964  ...    0.092368   \n",
       "50%      0.280438    0.235061    0.214349    0.244673  ...    0.151213   \n",
       "75%      0.407738    0.361852    0.334555    0.368082  ...    0.227175   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "               52          53          54          55          56          57  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.265172    0.290669    0.197061    0.200555    0.213642    0.175035   \n",
       "std      0.183385    0.213474    0.160717    0.147080    0.164361    0.148051   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.118831    0.127924    0.080499    0.102564    0.096591    0.075515   \n",
       "50%      0.235065    0.242690    0.156463    0.165385    0.160511    0.125858   \n",
       "75%      0.374026    0.394737    0.260771    0.260897    0.287642    0.229977   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               58          59          60  \n",
       "count  208.000000  208.000000  208.000000  \n",
       "mean     0.216015    0.136425    0.533654  \n",
       "std      0.170286    0.116190    0.500070  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.098485    0.057737    0.000000  \n",
       "50%      0.173554    0.108545    1.000000  \n",
       "75%      0.281680    0.183025    1.000000  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 61 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fd150",
   "metadata": {},
   "source": [
    "Train test val split\n",
    "80/10/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f752c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sonar_data.drop(columns=60, axis=1)\n",
    "y = sonar_data[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebe8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb690bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 60)\n",
      "1    91\n",
      "0    75\n",
      "Name: 60, dtype: int64\n",
      "******\n",
      "(21, 60)\n",
      "1    11\n",
      "0    10\n",
      "Name: 60, dtype: int64\n",
      "******\n",
      "(21, 60)\n",
      "0    12\n",
      "1     9\n",
      "Name: 60, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.value_counts())\n",
    "print('******')\n",
    "print(X_test.shape)\n",
    "print(y_test.value_counts())\n",
    "print('******')\n",
    "print(X_val.shape)\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc608bc5",
   "metadata": {},
   "source": [
    "Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbd99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1b105",
   "metadata": {},
   "source": [
    "Vytvorenie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4021b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = Sequential()\n",
    "tf_model.add(Dense(16, activation=tf.keras.activations.relu, input_dim=len(X_train.columns)))\n",
    "tf_model.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model.add(Dense(1, activation=tf.keras.activations.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a713898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce86c6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 20ms/step - loss: 0.6771 - accuracy: 0.5843 - val_loss: 0.6850 - val_accuracy: 0.4762\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6664 - accuracy: 0.6024 - val_loss: 0.6932 - val_accuracy: 0.4286\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6570 - accuracy: 0.5843 - val_loss: 0.6975 - val_accuracy: 0.4286\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6482 - accuracy: 0.6084 - val_loss: 0.6892 - val_accuracy: 0.4286\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.6366 - accuracy: 0.6265 - val_loss: 0.6818 - val_accuracy: 0.4286\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6260 - accuracy: 0.6928 - val_loss: 0.6632 - val_accuracy: 0.5714\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6161 - accuracy: 0.6807 - val_loss: 0.6716 - val_accuracy: 0.4762\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6008 - accuracy: 0.6928 - val_loss: 0.6486 - val_accuracy: 0.6190\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5902 - accuracy: 0.7711 - val_loss: 0.6451 - val_accuracy: 0.5714\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5776 - accuracy: 0.7289 - val_loss: 0.6466 - val_accuracy: 0.5238\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5610 - accuracy: 0.7590 - val_loss: 0.6263 - val_accuracy: 0.7143\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5463 - accuracy: 0.7831 - val_loss: 0.6292 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7771 - val_loss: 0.6217 - val_accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5155 - accuracy: 0.7831 - val_loss: 0.6114 - val_accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.8072 - val_loss: 0.6074 - val_accuracy: 0.7143\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4885 - accuracy: 0.8072 - val_loss: 0.5926 - val_accuracy: 0.7619\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4760 - accuracy: 0.7952 - val_loss: 0.6004 - val_accuracy: 0.7143\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4573 - accuracy: 0.8253 - val_loss: 0.5504 - val_accuracy: 0.8095\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4386 - accuracy: 0.8253 - val_loss: 0.5903 - val_accuracy: 0.7143\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.8012 - val_loss: 0.5544 - val_accuracy: 0.7143\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4175 - accuracy: 0.7892 - val_loss: 0.5360 - val_accuracy: 0.7619\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.4032 - accuracy: 0.8373 - val_loss: 0.5544 - val_accuracy: 0.7143\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3942 - accuracy: 0.8554 - val_loss: 0.5497 - val_accuracy: 0.7143\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3795 - accuracy: 0.8313 - val_loss: 0.5554 - val_accuracy: 0.7143\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3706 - accuracy: 0.8193 - val_loss: 0.5226 - val_accuracy: 0.7143\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3641 - accuracy: 0.8735 - val_loss: 0.5341 - val_accuracy: 0.7143\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3521 - accuracy: 0.8373 - val_loss: 0.5564 - val_accuracy: 0.7143\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8916 - val_loss: 0.5342 - val_accuracy: 0.7143\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3455 - accuracy: 0.8494 - val_loss: 0.5490 - val_accuracy: 0.7143\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3428 - accuracy: 0.8795 - val_loss: 0.5614 - val_accuracy: 0.7143\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3205 - accuracy: 0.8614 - val_loss: 0.5272 - val_accuracy: 0.7619\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8916 - val_loss: 0.5480 - val_accuracy: 0.7619\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3069 - accuracy: 0.9096 - val_loss: 0.5481 - val_accuracy: 0.7143\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3074 - accuracy: 0.9036 - val_loss: 0.5562 - val_accuracy: 0.7619\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2962 - accuracy: 0.8855 - val_loss: 0.5439 - val_accuracy: 0.7619\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2909 - accuracy: 0.9096 - val_loss: 0.5237 - val_accuracy: 0.7143\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2838 - accuracy: 0.9036 - val_loss: 0.5498 - val_accuracy: 0.7619\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.8976 - val_loss: 0.5204 - val_accuracy: 0.7143\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2731 - accuracy: 0.9157 - val_loss: 0.5370 - val_accuracy: 0.7143\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2687 - accuracy: 0.9096 - val_loss: 0.5506 - val_accuracy: 0.7143\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2699 - accuracy: 0.9217 - val_loss: 0.5180 - val_accuracy: 0.7619\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.9337 - val_loss: 0.5686 - val_accuracy: 0.7619\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.9217 - val_loss: 0.5350 - val_accuracy: 0.7143\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2518 - accuracy: 0.9157 - val_loss: 0.5545 - val_accuracy: 0.7143\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2535 - accuracy: 0.8976 - val_loss: 0.5009 - val_accuracy: 0.7143\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2500 - accuracy: 0.9217 - val_loss: 0.5243 - val_accuracy: 0.7143\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2409 - accuracy: 0.9217 - val_loss: 0.5159 - val_accuracy: 0.7143\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.9277 - val_loss: 0.5551 - val_accuracy: 0.7143\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2382 - accuracy: 0.9337 - val_loss: 0.5672 - val_accuracy: 0.7143\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2512 - accuracy: 0.8795 - val_loss: 0.5437 - val_accuracy: 0.7143\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2563 - accuracy: 0.9036 - val_loss: 0.5855 - val_accuracy: 0.7619\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2268 - accuracy: 0.9157 - val_loss: 0.5627 - val_accuracy: 0.7143\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9217 - val_loss: 0.5232 - val_accuracy: 0.7619\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2160 - accuracy: 0.9277 - val_loss: 0.5903 - val_accuracy: 0.7143\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.9277 - val_loss: 0.5355 - val_accuracy: 0.7143\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2125 - accuracy: 0.9277 - val_loss: 0.5787 - val_accuracy: 0.7143\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2153 - accuracy: 0.9337 - val_loss: 0.5852 - val_accuracy: 0.7143\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2162 - accuracy: 0.9096 - val_loss: 0.5642 - val_accuracy: 0.7143\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9337 - val_loss: 0.5956 - val_accuracy: 0.7143\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2016 - accuracy: 0.9337 - val_loss: 0.5541 - val_accuracy: 0.7143\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2121 - accuracy: 0.9337 - val_loss: 0.5885 - val_accuracy: 0.7143\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.2206 - accuracy: 0.8855 - val_loss: 0.5276 - val_accuracy: 0.7619\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1975 - accuracy: 0.9337 - val_loss: 0.6083 - val_accuracy: 0.7143\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1920 - accuracy: 0.9337 - val_loss: 0.5850 - val_accuracy: 0.7143\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.9458 - val_loss: 0.5918 - val_accuracy: 0.7143\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1815 - accuracy: 0.9518 - val_loss: 0.5953 - val_accuracy: 0.7143\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1781 - accuracy: 0.9458 - val_loss: 0.6016 - val_accuracy: 0.7619\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9518 - val_loss: 0.5899 - val_accuracy: 0.7143\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9518 - val_loss: 0.5833 - val_accuracy: 0.7619\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1814 - accuracy: 0.9337 - val_loss: 0.5611 - val_accuracy: 0.7619\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1673 - accuracy: 0.9578 - val_loss: 0.6403 - val_accuracy: 0.7619\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1705 - accuracy: 0.9578 - val_loss: 0.5893 - val_accuracy: 0.7619\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9639 - val_loss: 0.5996 - val_accuracy: 0.7143\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9578 - val_loss: 0.5834 - val_accuracy: 0.7619\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1567 - accuracy: 0.9578 - val_loss: 0.6107 - val_accuracy: 0.7143\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1534 - accuracy: 0.9639 - val_loss: 0.5894 - val_accuracy: 0.7619\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9578 - val_loss: 0.5778 - val_accuracy: 0.7619\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1472 - accuracy: 0.9578 - val_loss: 0.5963 - val_accuracy: 0.7619\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1465 - accuracy: 0.9578 - val_loss: 0.6090 - val_accuracy: 0.7619\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1445 - accuracy: 0.9699 - val_loss: 0.6087 - val_accuracy: 0.7619\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.9699 - val_loss: 0.5926 - val_accuracy: 0.7619\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1402 - accuracy: 0.9699 - val_loss: 0.6020 - val_accuracy: 0.7619\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1378 - accuracy: 0.9699 - val_loss: 0.6234 - val_accuracy: 0.7619\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.9639 - val_loss: 0.5678 - val_accuracy: 0.7619\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1357 - accuracy: 0.9639 - val_loss: 0.6064 - val_accuracy: 0.7619\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.9578 - val_loss: 0.5755 - val_accuracy: 0.7619\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1392 - accuracy: 0.9578 - val_loss: 0.5494 - val_accuracy: 0.7619\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.9639 - val_loss: 0.6124 - val_accuracy: 0.7619\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1243 - accuracy: 0.9699 - val_loss: 0.5768 - val_accuracy: 0.7619\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1202 - accuracy: 0.9759 - val_loss: 0.5243 - val_accuracy: 0.7619\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1173 - accuracy: 0.9759 - val_loss: 0.5347 - val_accuracy: 0.7619\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1185 - accuracy: 0.9759 - val_loss: 0.5462 - val_accuracy: 0.7619\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1136 - accuracy: 0.9699 - val_loss: 0.5948 - val_accuracy: 0.7619\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9759 - val_loss: 0.5509 - val_accuracy: 0.7619\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9639 - val_loss: 0.5507 - val_accuracy: 0.7619\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9699 - val_loss: 0.5112 - val_accuracy: 0.7619\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1187 - accuracy: 0.9639 - val_loss: 0.6038 - val_accuracy: 0.7619\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9639 - val_loss: 0.4993 - val_accuracy: 0.7619\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0909 - accuracy: 0.9819 - val_loss: 0.6268 - val_accuracy: 0.7619\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.9699 - val_loss: 0.5483 - val_accuracy: 0.7619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x196f37cab80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=16,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val,y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b0e13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model\\assets\n"
     ]
    }
   ],
   "source": [
    "tf_model.save('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2085f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pomocna funkcia\n",
    "def predictEval(tf_model, XX, yy):\n",
    "    # vykonanie predikcie\n",
    "    y_pred = tf_model.predict(XX)\n",
    "    # uprava outputu na boolean\n",
    "    y_pred_bool = np.copy(y_pred)\n",
    "    for x in y_pred_bool:\n",
    "        x[0] = round(x[0])\n",
    "    y_pred_bool\n",
    "\n",
    "    #vratenie y a accuaracy\n",
    "    return [y_pred, y_pred_bool, accuracy_score(y_pred_bool, yy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ac23f",
   "metadata": {},
   "source": [
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b444aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1000us/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Accuracy score\n",
      "Train: 98.80%\n",
      "Val: 76.19%\n",
      "Test: 76.19%\n"
     ]
    }
   ],
   "source": [
    "train = predictEval(tf_model, X_train, y_train)\n",
    "val = predictEval(tf_model, X_val, y_val)\n",
    "test = predictEval(tf_model, X_test, y_test)\n",
    "\n",
    "print('Accuracy score')\n",
    "print(f'Train: {train[2]*100:.2f}%')\n",
    "print(f'Val: {val[2]*100:.2f}%')\n",
    "print(f'Test: {test[2]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a688b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f17a7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch model\n",
    "class PyTorchModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PyTorchModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d7e8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class SonarDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65da1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SonarDataset(X_train, y_train)\n",
    "val_dataset = SonarDataset(X_val, y_val)\n",
    "test_dataset = SonarDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06616cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_dim = len(X_train.columns)\n",
    "model = PyTorchModel(input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1b6f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9157ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Loss: 0.6938, Val Loss: 0.6985\n",
      "Epoch: 2/100, Loss: 0.6939, Val Loss: 0.6993\n",
      "Epoch: 3/100, Loss: 0.6931, Val Loss: 0.7001\n",
      "Epoch: 4/100, Loss: 0.6759, Val Loss: 0.6992\n",
      "Epoch: 5/100, Loss: 0.6705, Val Loss: 0.6991\n",
      "Epoch: 6/100, Loss: 0.6622, Val Loss: 0.6976\n",
      "Epoch: 7/100, Loss: 0.7124, Val Loss: 0.6966\n",
      "Epoch: 8/100, Loss: 0.6405, Val Loss: 0.6928\n",
      "Epoch: 9/100, Loss: 0.7340, Val Loss: 0.6968\n",
      "Epoch: 10/100, Loss: 0.6303, Val Loss: 0.6795\n",
      "Epoch: 11/100, Loss: 0.6216, Val Loss: 0.6760\n",
      "Epoch: 12/100, Loss: 0.6344, Val Loss: 0.6567\n",
      "Epoch: 13/100, Loss: 0.5790, Val Loss: 0.6380\n",
      "Epoch: 14/100, Loss: 0.6828, Val Loss: 0.6345\n",
      "Epoch: 15/100, Loss: 0.5233, Val Loss: 0.6132\n",
      "Epoch: 16/100, Loss: 0.4052, Val Loss: 0.6398\n",
      "Epoch: 17/100, Loss: 0.3434, Val Loss: 0.5785\n",
      "Epoch: 18/100, Loss: 0.4802, Val Loss: 0.6134\n",
      "Epoch: 19/100, Loss: 0.5932, Val Loss: 0.6040\n",
      "Epoch: 20/100, Loss: 0.4175, Val Loss: 0.5921\n",
      "Epoch: 21/100, Loss: 0.5626, Val Loss: 0.5845\n",
      "Epoch: 22/100, Loss: 0.6027, Val Loss: 0.5906\n",
      "Epoch: 23/100, Loss: 0.3904, Val Loss: 0.5925\n",
      "Epoch: 24/100, Loss: 0.2025, Val Loss: 0.5906\n",
      "Epoch: 25/100, Loss: 0.2419, Val Loss: 0.6054\n",
      "Epoch: 26/100, Loss: 0.3776, Val Loss: 0.5965\n",
      "Epoch: 27/100, Loss: 0.3315, Val Loss: 0.6441\n",
      "Epoch: 28/100, Loss: 0.1430, Val Loss: 0.6030\n",
      "Epoch: 29/100, Loss: 0.2810, Val Loss: 0.6411\n",
      "Epoch: 30/100, Loss: 0.5268, Val Loss: 0.6234\n",
      "Epoch: 31/100, Loss: 0.5725, Val Loss: 0.6488\n",
      "Epoch: 32/100, Loss: 0.3094, Val Loss: 0.6027\n",
      "Epoch: 33/100, Loss: 0.7447, Val Loss: 0.6621\n",
      "Epoch: 34/100, Loss: 0.3341, Val Loss: 0.6126\n",
      "Epoch: 35/100, Loss: 0.2747, Val Loss: 0.6406\n",
      "Epoch: 36/100, Loss: 0.2410, Val Loss: 0.6320\n",
      "Epoch: 37/100, Loss: 0.1043, Val Loss: 0.6322\n",
      "Epoch: 38/100, Loss: 0.7040, Val Loss: 0.6645\n",
      "Epoch: 39/100, Loss: 0.2086, Val Loss: 0.6317\n",
      "Epoch: 40/100, Loss: 0.0727, Val Loss: 0.6464\n",
      "Epoch: 41/100, Loss: 0.2146, Val Loss: 0.6335\n",
      "Epoch: 42/100, Loss: 0.2875, Val Loss: 0.6455\n",
      "Epoch: 43/100, Loss: 0.6711, Val Loss: 0.6621\n",
      "Epoch: 44/100, Loss: 0.4070, Val Loss: 0.6433\n",
      "Epoch: 45/100, Loss: 0.9617, Val Loss: 0.6508\n",
      "Epoch: 46/100, Loss: 0.3418, Val Loss: 0.6398\n",
      "Epoch: 47/100, Loss: 0.5489, Val Loss: 0.6428\n",
      "Epoch: 48/100, Loss: 0.1481, Val Loss: 0.6449\n",
      "Epoch: 49/100, Loss: 0.2285, Val Loss: 0.6453\n",
      "Epoch: 50/100, Loss: 0.2094, Val Loss: 0.6567\n",
      "Epoch: 51/100, Loss: 0.6318, Val Loss: 0.6509\n",
      "Epoch: 52/100, Loss: 0.4227, Val Loss: 0.6503\n",
      "Epoch: 53/100, Loss: 0.8434, Val Loss: 0.6651\n",
      "Epoch: 54/100, Loss: 0.2923, Val Loss: 0.6510\n",
      "Epoch: 55/100, Loss: 0.3626, Val Loss: 0.6551\n",
      "Epoch: 56/100, Loss: 0.4161, Val Loss: 0.6401\n",
      "Epoch: 57/100, Loss: 0.2920, Val Loss: 0.6400\n",
      "Epoch: 58/100, Loss: 0.3311, Val Loss: 0.6412\n",
      "Epoch: 59/100, Loss: 0.1931, Val Loss: 0.6479\n",
      "Epoch: 60/100, Loss: 0.0721, Val Loss: 0.6637\n",
      "Epoch: 61/100, Loss: 0.4043, Val Loss: 0.6446\n",
      "Epoch: 62/100, Loss: 0.2305, Val Loss: 0.6686\n",
      "Epoch: 63/100, Loss: 0.0779, Val Loss: 0.6851\n",
      "Epoch: 64/100, Loss: 0.0955, Val Loss: 0.6832\n",
      "Epoch: 65/100, Loss: 0.1514, Val Loss: 0.6705\n",
      "Epoch: 66/100, Loss: 0.0436, Val Loss: 0.6926\n",
      "Epoch: 67/100, Loss: 0.4639, Val Loss: 0.6906\n",
      "Epoch: 68/100, Loss: 0.3624, Val Loss: 0.6953\n",
      "Epoch: 69/100, Loss: 0.0796, Val Loss: 0.6782\n",
      "Epoch: 70/100, Loss: 0.1036, Val Loss: 0.6890\n",
      "Epoch: 71/100, Loss: 0.2239, Val Loss: 0.6998\n",
      "Epoch: 72/100, Loss: 0.0500, Val Loss: 0.7128\n",
      "Epoch: 73/100, Loss: 0.1806, Val Loss: 0.7263\n",
      "Epoch: 74/100, Loss: 0.6598, Val Loss: 0.7181\n",
      "Epoch: 75/100, Loss: 0.1946, Val Loss: 0.7291\n",
      "Epoch: 76/100, Loss: 0.0875, Val Loss: 0.7337\n",
      "Epoch: 77/100, Loss: 0.3918, Val Loss: 0.7414\n",
      "Epoch: 78/100, Loss: 0.2588, Val Loss: 0.7446\n",
      "Epoch: 79/100, Loss: 0.1894, Val Loss: 0.7393\n",
      "Epoch: 80/100, Loss: 0.1724, Val Loss: 0.7563\n",
      "Epoch: 81/100, Loss: 0.0358, Val Loss: 0.7573\n",
      "Epoch: 82/100, Loss: 0.2793, Val Loss: 0.7607\n",
      "Epoch: 83/100, Loss: 0.1913, Val Loss: 0.7822\n",
      "Epoch: 84/100, Loss: 0.0135, Val Loss: 0.7832\n",
      "Epoch: 85/100, Loss: 0.2095, Val Loss: 0.7824\n",
      "Epoch: 86/100, Loss: 0.1553, Val Loss: 0.7926\n",
      "Epoch: 87/100, Loss: 0.1482, Val Loss: 0.8001\n",
      "Epoch: 88/100, Loss: 0.2719, Val Loss: 0.7980\n",
      "Epoch: 89/100, Loss: 0.1404, Val Loss: 0.8107\n",
      "Epoch: 90/100, Loss: 0.2647, Val Loss: 0.8328\n",
      "Epoch: 91/100, Loss: 0.2765, Val Loss: 0.8216\n",
      "Epoch: 92/100, Loss: 0.0500, Val Loss: 0.8223\n",
      "Epoch: 93/100, Loss: 0.0670, Val Loss: 0.8303\n",
      "Epoch: 94/100, Loss: 0.6278, Val Loss: 0.8313\n",
      "Epoch: 95/100, Loss: 0.1685, Val Loss: 0.8173\n",
      "Epoch: 96/100, Loss: 0.5168, Val Loss: 0.8270\n",
      "Epoch: 97/100, Loss: 0.0572, Val Loss: 0.8614\n",
      "Epoch: 98/100, Loss: 0.0469, Val Loss: 0.8226\n",
      "Epoch: 99/100, Loss: 0.0934, Val Loss: 0.8521\n",
      "Epoch: 100/100, Loss: 0.1197, Val Loss: 0.8487\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ab49a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_eval(model, loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            y_pred_bool = torch.round(y_pred)\n",
    "            accuracy = accuracy_score(y_pred_bool, y_batch)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce38c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = predict_eval(model, train_loader)\n",
    "val_accuracy = predict_eval(model, val_loader)\n",
    "test_accuracy = predict_eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5df66fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score\n",
      "Train: 83.33%\n",
      "Val: 71.43%\n",
      "Test: 76.19%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score')\n",
    "print(f'Train: {train_accuracy * 100:.2f}%')\n",
    "print(f'Val: {val_accuracy * 100:.2f}%')\n",
    "print(f'Test: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db5e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
