{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "cb8c8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d1fad",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "399516fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0       1       2       3       4   ...      56      57      58      59  60\n",
      "0  0.0200  0.0371  0.0428  0.0207  0.0954  ...  0.0180  0.0084  0.0090  0.0032   R\n",
      "1  0.0453  0.0523  0.0843  0.0689  0.1183  ...  0.0140  0.0049  0.0052  0.0044   R\n",
      "2  0.0262  0.0582  0.1099  0.1083  0.0974  ...  0.0316  0.0164  0.0095  0.0078   R\n",
      "3  0.0100  0.0171  0.0623  0.0205  0.0205  ...  0.0050  0.0044  0.0040  0.0117   R\n",
      "4  0.0762  0.0666  0.0481  0.0394  0.0590  ...  0.0072  0.0048  0.0107  0.0094   R\n",
      "\n",
      "[5 rows x 61 columns]\n",
      "(208, 61)\n",
      "M    111\n",
      "R     97\n",
      "Name: 60, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sonar_data = pd.read_csv('dataset/sonar.all-data', header=None)\n",
    "print(sonar_data.head())\n",
    "print(sonar_data.shape)\n",
    "print(sonar_data[60].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "ad1be166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>0.236013</td>\n",
       "      <td>0.250221</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.296568</td>\n",
       "      <td>0.320201</td>\n",
       "      <td>0.378487</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.452318</td>\n",
       "      <td>0.504812</td>\n",
       "      <td>0.563047</td>\n",
       "      <td>0.609060</td>\n",
       "      <td>0.624275</td>\n",
       "      <td>0.646975</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.675424</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.702155</td>\n",
       "      <td>0.694024</td>\n",
       "      <td>0.642074</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.504475</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.417220</td>\n",
       "      <td>0.403233</td>\n",
       "      <td>0.392571</td>\n",
       "      <td>0.384848</td>\n",
       "      <td>0.363807</td>\n",
       "      <td>0.339657</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.311207</td>\n",
       "      <td>0.289252</td>\n",
       "      <td>0.278293</td>\n",
       "      <td>0.246542</td>\n",
       "      <td>0.214075</td>\n",
       "      <td>0.197232</td>\n",
       "      <td>0.160631</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>0.051929</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.205427</td>\n",
       "      <td>0.232650</td>\n",
       "      <td>0.263677</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.262653</td>\n",
       "      <td>0.257818</td>\n",
       "      <td>0.255883</td>\n",
       "      <td>0.250175</td>\n",
       "      <td>0.239116</td>\n",
       "      <td>0.244926</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.245657</td>\n",
       "      <td>0.237189</td>\n",
       "      <td>0.240250</td>\n",
       "      <td>0.220749</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.213237</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.259132</td>\n",
       "      <td>0.264121</td>\n",
       "      <td>0.239912</td>\n",
       "      <td>0.212973</td>\n",
       "      <td>0.199075</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.168728</td>\n",
       "      <td>0.138993</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.151628</td>\n",
       "      <td>0.133938</td>\n",
       "      <td>0.086953</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.035954</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.164625</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.242075</td>\n",
       "      <td>0.299075</td>\n",
       "      <td>0.350625</td>\n",
       "      <td>0.399725</td>\n",
       "      <td>0.406925</td>\n",
       "      <td>0.450225</td>\n",
       "      <td>0.540725</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.544175</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.534775</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.345550</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.257875</td>\n",
       "      <td>0.217575</td>\n",
       "      <td>0.179375</td>\n",
       "      <td>0.154350</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.174275</td>\n",
       "      <td>0.173975</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.126875</td>\n",
       "      <td>0.094475</td>\n",
       "      <td>0.068550</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.045125</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.249050</td>\n",
       "      <td>0.263950</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.304700</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.434950</td>\n",
       "      <td>0.542500</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.721100</td>\n",
       "      <td>0.754500</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.731900</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.607150</td>\n",
       "      <td>0.490350</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.351050</td>\n",
       "      <td>0.312750</td>\n",
       "      <td>0.321150</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.278050</td>\n",
       "      <td>0.259500</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.222550</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.121350</td>\n",
       "      <td>0.101650</td>\n",
       "      <td>0.078100</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.301650</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.386175</td>\n",
       "      <td>0.452925</td>\n",
       "      <td>0.535725</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>0.679050</td>\n",
       "      <td>0.731400</td>\n",
       "      <td>0.809325</td>\n",
       "      <td>0.816975</td>\n",
       "      <td>0.831975</td>\n",
       "      <td>0.848575</td>\n",
       "      <td>0.872175</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>0.900275</td>\n",
       "      <td>0.852125</td>\n",
       "      <td>0.735175</td>\n",
       "      <td>0.641950</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>0.596125</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>0.556525</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.440550</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.424350</td>\n",
       "      <td>0.387525</td>\n",
       "      <td>0.384250</td>\n",
       "      <td>0.324525</td>\n",
       "      <td>0.271750</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.200375</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.068525</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>0.734200</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965700</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.773300</td>\n",
       "      <td>0.776200</td>\n",
       "      <td>0.703400</td>\n",
       "      <td>0.729200</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2   ...          57          58          59\n",
       "count  208.000000  208.000000  208.000000  ...  208.000000  208.000000  208.000000\n",
       "mean     0.029164    0.038437    0.043832  ...    0.007949    0.007941    0.006507\n",
       "std      0.022991    0.032960    0.038428  ...    0.006470    0.006181    0.005031\n",
       "min      0.001500    0.000600    0.001500  ...    0.000300    0.000100    0.000600\n",
       "25%      0.013350    0.016450    0.018950  ...    0.003600    0.003675    0.003100\n",
       "50%      0.022800    0.030800    0.034300  ...    0.005800    0.006400    0.005300\n",
       "75%      0.035550    0.047950    0.057950  ...    0.010350    0.010325    0.008525\n",
       "max      0.137100    0.233900    0.305900  ...    0.044000    0.036400    0.043900\n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "32fd4bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>0.4943</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.7464</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3271</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.4182</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.3973</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4   ...      56      57      58      59  60\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  ...  0.0180  0.0084  0.0090  0.0032   0\n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  ...  0.0140  0.0049  0.0052  0.0044   0\n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  ...  0.0316  0.0164  0.0095  0.0078   0\n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  ...  0.0050  0.0044  0.0040  0.0117   0\n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  ...  0.0072  0.0048  0.0107  0.0094   0\n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacnutie R za 0 a M za 1\n",
    "# R - Rock M - Mina\n",
    "sonar_data[60] = sonar_data[60].replace(['R', 'M'], [0, 1])\n",
    "sonar_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90d065",
   "metadata": {},
   "source": [
    "Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "43fefc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.399551</td>\n",
       "      <td>-0.040648</td>\n",
       "      <td>-0.026926</td>\n",
       "      <td>-0.715105</td>\n",
       "      <td>0.364456</td>\n",
       "      <td>-0.101253</td>\n",
       "      <td>0.521638</td>\n",
       "      <td>0.297843</td>\n",
       "      <td>1.125272</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>-0.567382</td>\n",
       "      <td>-0.658541</td>\n",
       "      <td>-0.352043</td>\n",
       "      <td>-1.414373</td>\n",
       "      <td>-1.240416</td>\n",
       "      <td>-0.651413</td>\n",
       "      <td>-0.402913</td>\n",
       "      <td>-0.584202</td>\n",
       "      <td>0.011612</td>\n",
       "      <td>-0.318092</td>\n",
       "      <td>-0.119597</td>\n",
       "      <td>-0.459029</td>\n",
       "      <td>-0.858165</td>\n",
       "      <td>-0.493225</td>\n",
       "      <td>-0.017695</td>\n",
       "      <td>-0.246629</td>\n",
       "      <td>0.033645</td>\n",
       "      <td>0.481687</td>\n",
       "      <td>0.154486</td>\n",
       "      <td>-0.886521</td>\n",
       "      <td>-1.750890</td>\n",
       "      <td>-0.839777</td>\n",
       "      <td>0.460548</td>\n",
       "      <td>1.523579</td>\n",
       "      <td>1.783805</td>\n",
       "      <td>1.768039</td>\n",
       "      <td>1.276008</td>\n",
       "      <td>1.271024</td>\n",
       "      <td>0.848461</td>\n",
       "      <td>-0.206511</td>\n",
       "      <td>-1.395741</td>\n",
       "      <td>0.030339</td>\n",
       "      <td>0.259328</td>\n",
       "      <td>1.590771</td>\n",
       "      <td>0.442062</td>\n",
       "      <td>-0.164885</td>\n",
       "      <td>-0.200048</td>\n",
       "      <td>0.688588</td>\n",
       "      <td>-0.379978</td>\n",
       "      <td>0.878510</td>\n",
       "      <td>0.595283</td>\n",
       "      <td>-1.115432</td>\n",
       "      <td>-0.597604</td>\n",
       "      <td>0.680897</td>\n",
       "      <td>-0.295646</td>\n",
       "      <td>1.481635</td>\n",
       "      <td>1.763784</td>\n",
       "      <td>0.069870</td>\n",
       "      <td>0.171678</td>\n",
       "      <td>-0.658947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.703538</td>\n",
       "      <td>0.421630</td>\n",
       "      <td>1.055618</td>\n",
       "      <td>0.323330</td>\n",
       "      <td>0.777676</td>\n",
       "      <td>2.607217</td>\n",
       "      <td>1.522625</td>\n",
       "      <td>2.510982</td>\n",
       "      <td>1.318325</td>\n",
       "      <td>0.588706</td>\n",
       "      <td>1.932142</td>\n",
       "      <td>2.898195</td>\n",
       "      <td>2.976719</td>\n",
       "      <td>2.944515</td>\n",
       "      <td>2.079703</td>\n",
       "      <td>2.438336</td>\n",
       "      <td>2.220238</td>\n",
       "      <td>1.667619</td>\n",
       "      <td>1.156279</td>\n",
       "      <td>0.834871</td>\n",
       "      <td>-0.341605</td>\n",
       "      <td>-0.858218</td>\n",
       "      <td>-1.006818</td>\n",
       "      <td>-1.179062</td>\n",
       "      <td>-1.434182</td>\n",
       "      <td>-1.605136</td>\n",
       "      <td>-1.530425</td>\n",
       "      <td>-1.763702</td>\n",
       "      <td>-0.833533</td>\n",
       "      <td>-1.717061</td>\n",
       "      <td>-0.588706</td>\n",
       "      <td>-0.678535</td>\n",
       "      <td>-1.062150</td>\n",
       "      <td>-0.733178</td>\n",
       "      <td>-1.013395</td>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.082283</td>\n",
       "      <td>-1.101179</td>\n",
       "      <td>-0.714012</td>\n",
       "      <td>-0.640777</td>\n",
       "      <td>-0.713840</td>\n",
       "      <td>-1.306977</td>\n",
       "      <td>-0.767653</td>\n",
       "      <td>-0.385613</td>\n",
       "      <td>-0.893356</td>\n",
       "      <td>-1.050261</td>\n",
       "      <td>-0.800670</td>\n",
       "      <td>-0.276618</td>\n",
       "      <td>-0.307489</td>\n",
       "      <td>-1.050756</td>\n",
       "      <td>-0.297902</td>\n",
       "      <td>-0.522349</td>\n",
       "      <td>-0.256857</td>\n",
       "      <td>-0.843151</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>1.901046</td>\n",
       "      <td>1.070732</td>\n",
       "      <td>-0.472406</td>\n",
       "      <td>-0.444554</td>\n",
       "      <td>-0.419852</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.129229</td>\n",
       "      <td>0.601067</td>\n",
       "      <td>1.723404</td>\n",
       "      <td>1.172176</td>\n",
       "      <td>0.400545</td>\n",
       "      <td>2.093337</td>\n",
       "      <td>1.968770</td>\n",
       "      <td>2.852370</td>\n",
       "      <td>3.232767</td>\n",
       "      <td>3.066105</td>\n",
       "      <td>3.000992</td>\n",
       "      <td>3.261740</td>\n",
       "      <td>1.998926</td>\n",
       "      <td>1.434872</td>\n",
       "      <td>1.599057</td>\n",
       "      <td>1.355566</td>\n",
       "      <td>0.988117</td>\n",
       "      <td>1.160528</td>\n",
       "      <td>1.507916</td>\n",
       "      <td>1.140572</td>\n",
       "      <td>0.732277</td>\n",
       "      <td>0.193621</td>\n",
       "      <td>-0.872189</td>\n",
       "      <td>-1.290573</td>\n",
       "      <td>-0.582489</td>\n",
       "      <td>-1.937685</td>\n",
       "      <td>-0.796337</td>\n",
       "      <td>0.673135</td>\n",
       "      <td>-0.160528</td>\n",
       "      <td>1.228201</td>\n",
       "      <td>1.624174</td>\n",
       "      <td>0.307721</td>\n",
       "      <td>-1.121368</td>\n",
       "      <td>-0.573653</td>\n",
       "      <td>0.118483</td>\n",
       "      <td>-0.305701</td>\n",
       "      <td>1.035341</td>\n",
       "      <td>1.581199</td>\n",
       "      <td>1.065989</td>\n",
       "      <td>0.901598</td>\n",
       "      <td>1.027820</td>\n",
       "      <td>-0.116403</td>\n",
       "      <td>-0.242623</td>\n",
       "      <td>0.061104</td>\n",
       "      <td>0.091680</td>\n",
       "      <td>-1.070468</td>\n",
       "      <td>0.142341</td>\n",
       "      <td>-0.273406</td>\n",
       "      <td>-1.085353</td>\n",
       "      <td>-0.720654</td>\n",
       "      <td>-1.065875</td>\n",
       "      <td>1.017585</td>\n",
       "      <td>0.836373</td>\n",
       "      <td>-0.197833</td>\n",
       "      <td>1.231812</td>\n",
       "      <td>2.827246</td>\n",
       "      <td>4.120162</td>\n",
       "      <td>1.309360</td>\n",
       "      <td>0.252761</td>\n",
       "      <td>0.257582</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.835555</td>\n",
       "      <td>-0.648910</td>\n",
       "      <td>0.481740</td>\n",
       "      <td>-0.719414</td>\n",
       "      <td>-0.987079</td>\n",
       "      <td>-1.149364</td>\n",
       "      <td>-0.193816</td>\n",
       "      <td>-0.084747</td>\n",
       "      <td>-1.000852</td>\n",
       "      <td>-0.610469</td>\n",
       "      <td>-1.117292</td>\n",
       "      <td>-0.365128</td>\n",
       "      <td>-1.812688</td>\n",
       "      <td>-0.429479</td>\n",
       "      <td>-0.718781</td>\n",
       "      <td>-0.712596</td>\n",
       "      <td>-1.317974</td>\n",
       "      <td>-0.859403</td>\n",
       "      <td>-0.383932</td>\n",
       "      <td>-0.632571</td>\n",
       "      <td>-1.302345</td>\n",
       "      <td>-1.000030</td>\n",
       "      <td>-0.366125</td>\n",
       "      <td>-0.788353</td>\n",
       "      <td>-1.479201</td>\n",
       "      <td>-0.703408</td>\n",
       "      <td>-0.720439</td>\n",
       "      <td>-1.868090</td>\n",
       "      <td>-1.806948</td>\n",
       "      <td>-1.021841</td>\n",
       "      <td>0.569262</td>\n",
       "      <td>1.386586</td>\n",
       "      <td>0.945462</td>\n",
       "      <td>-0.232062</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>-0.317466</td>\n",
       "      <td>0.739521</td>\n",
       "      <td>2.549849</td>\n",
       "      <td>3.322838</td>\n",
       "      <td>3.397228</td>\n",
       "      <td>1.891327</td>\n",
       "      <td>1.320721</td>\n",
       "      <td>0.536988</td>\n",
       "      <td>0.798111</td>\n",
       "      <td>1.535517</td>\n",
       "      <td>1.532522</td>\n",
       "      <td>1.649083</td>\n",
       "      <td>1.062786</td>\n",
       "      <td>0.450859</td>\n",
       "      <td>0.658442</td>\n",
       "      <td>0.670411</td>\n",
       "      <td>-0.137365</td>\n",
       "      <td>-1.009341</td>\n",
       "      <td>0.557326</td>\n",
       "      <td>-0.111785</td>\n",
       "      <td>-0.161060</td>\n",
       "      <td>-0.488635</td>\n",
       "      <td>-0.549875</td>\n",
       "      <td>-0.639154</td>\n",
       "      <td>1.034640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.050790</td>\n",
       "      <td>0.856537</td>\n",
       "      <td>0.111327</td>\n",
       "      <td>-0.312227</td>\n",
       "      <td>-0.292365</td>\n",
       "      <td>-0.672796</td>\n",
       "      <td>-0.013735</td>\n",
       "      <td>1.317299</td>\n",
       "      <td>1.510531</td>\n",
       "      <td>1.772220</td>\n",
       "      <td>1.353527</td>\n",
       "      <td>1.037528</td>\n",
       "      <td>1.083001</td>\n",
       "      <td>0.712657</td>\n",
       "      <td>0.647036</td>\n",
       "      <td>0.664024</td>\n",
       "      <td>1.196069</td>\n",
       "      <td>0.640022</td>\n",
       "      <td>-1.171910</td>\n",
       "      <td>-0.379537</td>\n",
       "      <td>-0.755295</td>\n",
       "      <td>-0.764199</td>\n",
       "      <td>-0.296406</td>\n",
       "      <td>-0.556527</td>\n",
       "      <td>-1.470607</td>\n",
       "      <td>-1.991772</td>\n",
       "      <td>-0.010833</td>\n",
       "      <td>1.293121</td>\n",
       "      <td>0.351005</td>\n",
       "      <td>-0.492820</td>\n",
       "      <td>0.027284</td>\n",
       "      <td>0.502340</td>\n",
       "      <td>-0.626746</td>\n",
       "      <td>-1.322726</td>\n",
       "      <td>-0.763886</td>\n",
       "      <td>0.126202</td>\n",
       "      <td>0.403590</td>\n",
       "      <td>-0.084520</td>\n",
       "      <td>-0.216518</td>\n",
       "      <td>-0.382686</td>\n",
       "      <td>-0.535163</td>\n",
       "      <td>-0.201359</td>\n",
       "      <td>-0.446000</td>\n",
       "      <td>-0.977475</td>\n",
       "      <td>-0.846418</td>\n",
       "      <td>-0.807026</td>\n",
       "      <td>-1.000109</td>\n",
       "      <td>-1.331760</td>\n",
       "      <td>-0.806549</td>\n",
       "      <td>-1.160790</td>\n",
       "      <td>-0.039129</td>\n",
       "      <td>-1.073812</td>\n",
       "      <td>-0.753780</td>\n",
       "      <td>-0.060532</td>\n",
       "      <td>0.241793</td>\n",
       "      <td>-1.174638</td>\n",
       "      <td>-0.107456</td>\n",
       "      <td>-0.487900</td>\n",
       "      <td>0.447361</td>\n",
       "      <td>0.576375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>-0.456232</td>\n",
       "      <td>-0.116681</td>\n",
       "      <td>-0.705146</td>\n",
       "      <td>-0.779738</td>\n",
       "      <td>-0.647842</td>\n",
       "      <td>0.990954</td>\n",
       "      <td>1.314965</td>\n",
       "      <td>0.407323</td>\n",
       "      <td>0.463980</td>\n",
       "      <td>0.448504</td>\n",
       "      <td>0.564920</td>\n",
       "      <td>0.308290</td>\n",
       "      <td>-0.325732</td>\n",
       "      <td>-1.201670</td>\n",
       "      <td>-0.683647</td>\n",
       "      <td>-0.682867</td>\n",
       "      <td>-0.542814</td>\n",
       "      <td>-0.636713</td>\n",
       "      <td>-1.171133</td>\n",
       "      <td>-1.484794</td>\n",
       "      <td>-0.762294</td>\n",
       "      <td>0.249249</td>\n",
       "      <td>0.661229</td>\n",
       "      <td>0.723758</td>\n",
       "      <td>0.885975</td>\n",
       "      <td>0.972436</td>\n",
       "      <td>1.210064</td>\n",
       "      <td>1.293121</td>\n",
       "      <td>0.702319</td>\n",
       "      <td>0.176971</td>\n",
       "      <td>0.466675</td>\n",
       "      <td>0.543708</td>\n",
       "      <td>-0.005920</td>\n",
       "      <td>-1.109882</td>\n",
       "      <td>-1.190178</td>\n",
       "      <td>-1.229851</td>\n",
       "      <td>-1.115201</td>\n",
       "      <td>-0.934560</td>\n",
       "      <td>-0.796592</td>\n",
       "      <td>-1.072800</td>\n",
       "      <td>-0.990935</td>\n",
       "      <td>-1.037850</td>\n",
       "      <td>-0.352245</td>\n",
       "      <td>-0.602955</td>\n",
       "      <td>-0.707588</td>\n",
       "      <td>-0.390159</td>\n",
       "      <td>0.341779</td>\n",
       "      <td>0.720708</td>\n",
       "      <td>0.944343</td>\n",
       "      <td>0.629099</td>\n",
       "      <td>0.353205</td>\n",
       "      <td>-0.189390</td>\n",
       "      <td>-0.129077</td>\n",
       "      <td>1.230104</td>\n",
       "      <td>-0.847228</td>\n",
       "      <td>0.328253</td>\n",
       "      <td>-0.228741</td>\n",
       "      <td>0.550172</td>\n",
       "      <td>1.841992</td>\n",
       "      <td>1.831621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.136733</td>\n",
       "      <td>-0.861801</td>\n",
       "      <td>-0.366036</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>0.014392</td>\n",
       "      <td>-0.148740</td>\n",
       "      <td>-0.369029</td>\n",
       "      <td>-0.388465</td>\n",
       "      <td>-0.635067</td>\n",
       "      <td>0.053253</td>\n",
       "      <td>0.547546</td>\n",
       "      <td>0.660385</td>\n",
       "      <td>0.182721</td>\n",
       "      <td>-0.953009</td>\n",
       "      <td>-0.959836</td>\n",
       "      <td>-0.969824</td>\n",
       "      <td>-0.858733</td>\n",
       "      <td>-0.802677</td>\n",
       "      <td>-1.137718</td>\n",
       "      <td>-1.234813</td>\n",
       "      <td>-1.095889</td>\n",
       "      <td>-0.115075</td>\n",
       "      <td>0.734153</td>\n",
       "      <td>0.971933</td>\n",
       "      <td>1.282558</td>\n",
       "      <td>1.231039</td>\n",
       "      <td>0.990123</td>\n",
       "      <td>1.293121</td>\n",
       "      <td>1.114550</td>\n",
       "      <td>0.727788</td>\n",
       "      <td>1.239590</td>\n",
       "      <td>1.281755</td>\n",
       "      <td>0.549377</td>\n",
       "      <td>-0.824211</td>\n",
       "      <td>-1.159618</td>\n",
       "      <td>-0.968356</td>\n",
       "      <td>-1.035815</td>\n",
       "      <td>-1.021634</td>\n",
       "      <td>-0.750266</td>\n",
       "      <td>-1.552513</td>\n",
       "      <td>-0.779452</td>\n",
       "      <td>-1.163799</td>\n",
       "      <td>-0.949394</td>\n",
       "      <td>-0.335978</td>\n",
       "      <td>-0.673211</td>\n",
       "      <td>-1.142316</td>\n",
       "      <td>-0.500936</td>\n",
       "      <td>0.547260</td>\n",
       "      <td>0.356065</td>\n",
       "      <td>-0.185154</td>\n",
       "      <td>-0.915619</td>\n",
       "      <td>-0.761663</td>\n",
       "      <td>-0.200066</td>\n",
       "      <td>0.351373</td>\n",
       "      <td>-0.422934</td>\n",
       "      <td>-0.335815</td>\n",
       "      <td>-0.765856</td>\n",
       "      <td>-0.735798</td>\n",
       "      <td>-0.282388</td>\n",
       "      <td>0.038412</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1.004381</td>\n",
       "      <td>0.160078</td>\n",
       "      <td>-0.673843</td>\n",
       "      <td>-0.531979</td>\n",
       "      <td>-0.723629</td>\n",
       "      <td>0.212502</td>\n",
       "      <td>0.064137</td>\n",
       "      <td>-0.200113</td>\n",
       "      <td>-0.442014</td>\n",
       "      <td>0.332912</td>\n",
       "      <td>0.268814</td>\n",
       "      <td>-0.091753</td>\n",
       "      <td>-0.608047</td>\n",
       "      <td>-1.208374</td>\n",
       "      <td>-1.229193</td>\n",
       "      <td>-0.983181</td>\n",
       "      <td>-0.926402</td>\n",
       "      <td>-0.837173</td>\n",
       "      <td>-1.199109</td>\n",
       "      <td>-1.616463</td>\n",
       "      <td>-1.137880</td>\n",
       "      <td>-0.138972</td>\n",
       "      <td>0.465294</td>\n",
       "      <td>0.732142</td>\n",
       "      <td>1.092657</td>\n",
       "      <td>1.262308</td>\n",
       "      <td>1.092544</td>\n",
       "      <td>1.293121</td>\n",
       "      <td>0.921786</td>\n",
       "      <td>0.531165</td>\n",
       "      <td>1.251770</td>\n",
       "      <td>1.370132</td>\n",
       "      <td>0.497439</td>\n",
       "      <td>-0.973766</td>\n",
       "      <td>-1.093856</td>\n",
       "      <td>-0.911806</td>\n",
       "      <td>-1.074672</td>\n",
       "      <td>-0.920910</td>\n",
       "      <td>-0.669197</td>\n",
       "      <td>-1.563734</td>\n",
       "      <td>-0.821632</td>\n",
       "      <td>-1.458472</td>\n",
       "      <td>-1.390765</td>\n",
       "      <td>-0.625516</td>\n",
       "      <td>-0.702299</td>\n",
       "      <td>-0.635639</td>\n",
       "      <td>-0.190826</td>\n",
       "      <td>0.145760</td>\n",
       "      <td>0.043804</td>\n",
       "      <td>0.225641</td>\n",
       "      <td>-0.047477</td>\n",
       "      <td>0.268428</td>\n",
       "      <td>-1.108725</td>\n",
       "      <td>-0.801960</td>\n",
       "      <td>-0.437077</td>\n",
       "      <td>0.118548</td>\n",
       "      <td>1.070732</td>\n",
       "      <td>0.906526</td>\n",
       "      <td>-0.039138</td>\n",
       "      <td>-0.678871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.049533</td>\n",
       "      <td>-0.095392</td>\n",
       "      <td>0.134804</td>\n",
       "      <td>0.148821</td>\n",
       "      <td>-1.055648</td>\n",
       "      <td>0.522865</td>\n",
       "      <td>0.401585</td>\n",
       "      <td>-0.264859</td>\n",
       "      <td>0.139685</td>\n",
       "      <td>0.202404</td>\n",
       "      <td>0.406292</td>\n",
       "      <td>0.221697</td>\n",
       "      <td>-0.821384</td>\n",
       "      <td>-1.641094</td>\n",
       "      <td>-1.234073</td>\n",
       "      <td>-1.008602</td>\n",
       "      <td>-0.794485</td>\n",
       "      <td>-0.719886</td>\n",
       "      <td>-0.862625</td>\n",
       "      <td>-0.510443</td>\n",
       "      <td>-0.157700</td>\n",
       "      <td>0.292732</td>\n",
       "      <td>0.311031</td>\n",
       "      <td>0.375389</td>\n",
       "      <td>1.018169</td>\n",
       "      <td>1.268223</td>\n",
       "      <td>1.206799</td>\n",
       "      <td>0.573396</td>\n",
       "      <td>0.255041</td>\n",
       "      <td>0.604728</td>\n",
       "      <td>0.865780</td>\n",
       "      <td>0.268233</td>\n",
       "      <td>-0.770910</td>\n",
       "      <td>-1.327928</td>\n",
       "      <td>-1.218417</td>\n",
       "      <td>-1.322456</td>\n",
       "      <td>-0.863253</td>\n",
       "      <td>-0.740641</td>\n",
       "      <td>-0.961248</td>\n",
       "      <td>-1.269734</td>\n",
       "      <td>-1.406286</td>\n",
       "      <td>-0.841204</td>\n",
       "      <td>-0.658752</td>\n",
       "      <td>-0.445777</td>\n",
       "      <td>-0.431910</td>\n",
       "      <td>-0.465000</td>\n",
       "      <td>0.038586</td>\n",
       "      <td>0.063854</td>\n",
       "      <td>-0.084446</td>\n",
       "      <td>0.269654</td>\n",
       "      <td>-0.990747</td>\n",
       "      <td>-0.501539</td>\n",
       "      <td>-0.867363</td>\n",
       "      <td>0.227802</td>\n",
       "      <td>-0.804798</td>\n",
       "      <td>-0.825128</td>\n",
       "      <td>-0.765856</td>\n",
       "      <td>-0.007598</td>\n",
       "      <td>-0.704020</td>\n",
       "      <td>-0.340154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-0.137949</td>\n",
       "      <td>-0.064979</td>\n",
       "      <td>-0.788619</td>\n",
       "      <td>-0.575067</td>\n",
       "      <td>-0.970839</td>\n",
       "      <td>-1.200244</td>\n",
       "      <td>-0.912514</td>\n",
       "      <td>0.061226</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.202404</td>\n",
       "      <td>0.271836</td>\n",
       "      <td>-0.043090</td>\n",
       "      <td>-0.759516</td>\n",
       "      <td>-1.602697</td>\n",
       "      <td>-0.927142</td>\n",
       "      <td>-0.894853</td>\n",
       "      <td>-0.753807</td>\n",
       "      <td>-0.516743</td>\n",
       "      <td>-0.518370</td>\n",
       "      <td>-0.411596</td>\n",
       "      <td>-0.151868</td>\n",
       "      <td>0.452173</td>\n",
       "      <td>0.637989</td>\n",
       "      <td>0.884736</td>\n",
       "      <td>1.095931</td>\n",
       "      <td>1.268223</td>\n",
       "      <td>0.960335</td>\n",
       "      <td>0.280097</td>\n",
       "      <td>0.293009</td>\n",
       "      <td>1.157362</td>\n",
       "      <td>1.207268</td>\n",
       "      <td>0.083017</td>\n",
       "      <td>-1.192722</td>\n",
       "      <td>-1.076503</td>\n",
       "      <td>-0.883805</td>\n",
       "      <td>-0.751266</td>\n",
       "      <td>-0.410750</td>\n",
       "      <td>-0.792416</td>\n",
       "      <td>-1.157627</td>\n",
       "      <td>-1.315181</td>\n",
       "      <td>-1.198317</td>\n",
       "      <td>-0.419988</td>\n",
       "      <td>0.028546</td>\n",
       "      <td>-0.209634</td>\n",
       "      <td>-0.352578</td>\n",
       "      <td>-0.102018</td>\n",
       "      <td>-0.269218</td>\n",
       "      <td>-1.402424</td>\n",
       "      <td>-1.018440</td>\n",
       "      <td>-0.647297</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>0.122759</td>\n",
       "      <td>0.311055</td>\n",
       "      <td>-0.856881</td>\n",
       "      <td>-0.762369</td>\n",
       "      <td>-0.370766</td>\n",
       "      <td>-0.661898</td>\n",
       "      <td>-0.673823</td>\n",
       "      <td>-0.298604</td>\n",
       "      <td>0.994790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3   ...        57        58        59  60\n",
       "0   -0.399551 -0.040648 -0.026926 -0.715105  ...  0.069870  0.171678 -0.658947   0\n",
       "1    0.703538  0.421630  1.055618  0.323330  ... -0.472406 -0.444554 -0.419852   0\n",
       "2   -0.129229  0.601067  1.723404  1.172176  ...  1.309360  0.252761  0.257582   0\n",
       "3   -0.835555 -0.648910  0.481740 -0.719414  ... -0.549875 -0.639154  1.034640   0\n",
       "4    2.050790  0.856537  0.111327 -0.312227  ... -0.487900  0.447361  0.576375   0\n",
       "..        ...       ...       ...       ...  ...       ...       ...       ...  ..\n",
       "203 -0.456232 -0.116681 -0.705146 -0.779738  ...  0.550172  1.841992  1.831621   1\n",
       "204  0.136733 -0.861801 -0.366036  0.054026  ... -0.735798 -0.282388  0.038412   1\n",
       "205  1.004381  0.160078 -0.673843 -0.531979  ...  0.906526 -0.039138 -0.678871   1\n",
       "206  0.049533 -0.095392  0.134804  0.148821  ... -0.007598 -0.704020 -0.340154   1\n",
       "207 -0.137949 -0.064979 -0.788619 -0.575067  ... -0.673823 -0.298604  0.994790   1\n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_dfMinMax = sonar_data.copy()\n",
    "for x in range(60):\n",
    "    normalized_dfMinMax[x] = MinMaxScaler().fit_transform(np.array(normalized_dfMinMax[x]).reshape(-1,1))\n",
    "\n",
    "normalized_dfMinMax\n",
    "\n",
    "normalized_df = sonar_data.copy()\n",
    "for x in range(60):\n",
    "    normalized_df[x] = StandardScaler().fit_transform(np.array(normalized_df[x]).reshape(-1,1))\n",
    "\n",
    "normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "8cb29279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.190024e-17</td>\n",
       "      <td>1.663333e-16</td>\n",
       "      <td>-9.661075e-17</td>\n",
       "      <td>1.627971e-16</td>\n",
       "      <td>-1.297039e-16</td>\n",
       "      <td>2.017617e-16</td>\n",
       "      <td>9.607699e-17</td>\n",
       "      <td>1.271019e-17</td>\n",
       "      <td>-3.339343e-16</td>\n",
       "      <td>2.268485e-17</td>\n",
       "      <td>-3.149190e-17</td>\n",
       "      <td>-1.731387e-17</td>\n",
       "      <td>6.725389e-17</td>\n",
       "      <td>-5.124106e-17</td>\n",
       "      <td>-6.138252e-17</td>\n",
       "      <td>-7.419279e-17</td>\n",
       "      <td>3.816392e-16</td>\n",
       "      <td>1.601283e-17</td>\n",
       "      <td>-1.200962e-16</td>\n",
       "      <td>7.018958e-17</td>\n",
       "      <td>4.546310e-16</td>\n",
       "      <td>-6.698701e-17</td>\n",
       "      <td>-3.234592e-16</td>\n",
       "      <td>4.483593e-17</td>\n",
       "      <td>-7.846288e-16</td>\n",
       "      <td>6.746740e-16</td>\n",
       "      <td>-2.556716e-16</td>\n",
       "      <td>-1.868164e-17</td>\n",
       "      <td>9.260755e-17</td>\n",
       "      <td>2.177745e-16</td>\n",
       "      <td>-1.174274e-17</td>\n",
       "      <td>1.694024e-16</td>\n",
       "      <td>4.270089e-18</td>\n",
       "      <td>-2.135044e-17</td>\n",
       "      <td>1.142249e-16</td>\n",
       "      <td>2.722181e-17</td>\n",
       "      <td>2.698162e-16</td>\n",
       "      <td>-1.323727e-16</td>\n",
       "      <td>7.792912e-17</td>\n",
       "      <td>3.373370e-16</td>\n",
       "      <td>-1.323727e-16</td>\n",
       "      <td>-9.073938e-17</td>\n",
       "      <td>5.439359e-17</td>\n",
       "      <td>3.855089e-16</td>\n",
       "      <td>-2.481989e-17</td>\n",
       "      <td>2.339208e-16</td>\n",
       "      <td>1.766749e-16</td>\n",
       "      <td>3.917806e-16</td>\n",
       "      <td>-2.135044e-17</td>\n",
       "      <td>-2.914335e-16</td>\n",
       "      <td>3.736327e-18</td>\n",
       "      <td>1.316388e-16</td>\n",
       "      <td>-1.387779e-17</td>\n",
       "      <td>-1.793437e-16</td>\n",
       "      <td>8.433425e-17</td>\n",
       "      <td>-2.199096e-16</td>\n",
       "      <td>4.590345e-17</td>\n",
       "      <td>-7.686159e-17</td>\n",
       "      <td>1.283695e-16</td>\n",
       "      <td>3.149190e-17</td>\n",
       "      <td>0.533654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>0.500070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.206158e+00</td>\n",
       "      <td>-1.150725e+00</td>\n",
       "      <td>-1.104253e+00</td>\n",
       "      <td>-1.036115e+00</td>\n",
       "      <td>-1.236093e+00</td>\n",
       "      <td>-1.600493e+00</td>\n",
       "      <td>-1.921613e+00</td>\n",
       "      <td>-1.522110e+00</td>\n",
       "      <td>-1.443689e+00</td>\n",
       "      <td>-1.468833e+00</td>\n",
       "      <td>-1.564472e+00</td>\n",
       "      <td>-1.621794e+00</td>\n",
       "      <td>-1.812688e+00</td>\n",
       "      <td>-1.641094e+00</td>\n",
       "      <td>-1.547347e+00</td>\n",
       "      <td>-1.560974e+00</td>\n",
       "      <td>-1.448751e+00</td>\n",
       "      <td>-1.589952e+00</td>\n",
       "      <td>-1.769499e+00</td>\n",
       "      <td>-1.898502e+00</td>\n",
       "      <td>-2.168994e+00</td>\n",
       "      <td>-2.359782e+00</td>\n",
       "      <td>-2.366740e+00</td>\n",
       "      <td>-2.719680e+00</td>\n",
       "      <td>-2.666087e+00</td>\n",
       "      <td>-2.568134e+00</td>\n",
       "      <td>-2.668895e+00</td>\n",
       "      <td>-2.813072e+00</td>\n",
       "      <td>-2.618893e+00</td>\n",
       "      <td>-2.359606e+00</td>\n",
       "      <td>-2.137348e+00</td>\n",
       "      <td>-1.873982e+00</td>\n",
       "      <td>-1.793647e+00</td>\n",
       "      <td>-1.656081e+00</td>\n",
       "      <td>-1.432336e+00</td>\n",
       "      <td>-1.430241e+00</td>\n",
       "      <td>-1.373417e+00</td>\n",
       "      <td>-1.418414e+00</td>\n",
       "      <td>-1.453707e+00</td>\n",
       "      <td>-1.680436e+00</td>\n",
       "      <td>-1.483614e+00</td>\n",
       "      <td>-1.620067e+00</td>\n",
       "      <td>-1.778046e+00</td>\n",
       "      <td>-1.609948e+00</td>\n",
       "      <td>-1.303898e+00</td>\n",
       "      <td>-1.202190e+00</td>\n",
       "      <td>-1.411667e+00</td>\n",
       "      <td>-1.468270e+00</td>\n",
       "      <td>-1.447799e+00</td>\n",
       "      <td>-1.498229e+00</td>\n",
       "      <td>-1.341343e+00</td>\n",
       "      <td>-1.313126e+00</td>\n",
       "      <td>-1.449472e+00</td>\n",
       "      <td>-1.364897e+00</td>\n",
       "      <td>-1.229092e+00</td>\n",
       "      <td>-1.366868e+00</td>\n",
       "      <td>-1.302971e+00</td>\n",
       "      <td>-1.185113e+00</td>\n",
       "      <td>-1.271603e+00</td>\n",
       "      <td>-1.176985e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.894939e-01</td>\n",
       "      <td>-6.686781e-01</td>\n",
       "      <td>-6.490624e-01</td>\n",
       "      <td>-6.359298e-01</td>\n",
       "      <td>-6.703975e-01</td>\n",
       "      <td>-6.367565e-01</td>\n",
       "      <td>-6.626732e-01</td>\n",
       "      <td>-6.400918e-01</td>\n",
       "      <td>-6.856590e-01</td>\n",
       "      <td>-7.232644e-01</td>\n",
       "      <td>-8.064569e-01</td>\n",
       "      <td>-8.354833e-01</td>\n",
       "      <td>-7.621827e-01</td>\n",
       "      <td>-7.398484e-01</td>\n",
       "      <td>-7.591598e-01</td>\n",
       "      <td>-7.849821e-01</td>\n",
       "      <td>-7.988564e-01</td>\n",
       "      <td>-8.058388e-01</td>\n",
       "      <td>-7.993884e-01</td>\n",
       "      <td>-8.107057e-01</td>\n",
       "      <td>-8.139075e-01</td>\n",
       "      <td>-8.514605e-01</td>\n",
       "      <td>-7.883456e-01</td>\n",
       "      <td>-5.530684e-01</td>\n",
       "      <td>-6.123656e-01</td>\n",
       "      <td>-6.578782e-01</td>\n",
       "      <td>-6.947312e-01</td>\n",
       "      <td>-6.730211e-01</td>\n",
       "      <td>-7.442437e-01</td>\n",
       "      <td>-7.698181e-01</td>\n",
       "      <td>-7.444604e-01</td>\n",
       "      <td>-7.410570e-01</td>\n",
       "      <td>-7.734584e-01</td>\n",
       "      <td>-8.048124e-01</td>\n",
       "      <td>-8.247161e-01</td>\n",
       "      <td>-8.748025e-01</td>\n",
       "      <td>-8.511364e-01</td>\n",
       "      <td>-7.784131e-01</td>\n",
       "      <td>-7.644914e-01</td>\n",
       "      <td>-6.999700e-01</td>\n",
       "      <td>-7.390302e-01</td>\n",
       "      <td>-7.093139e-01</td>\n",
       "      <td>-6.587522e-01</td>\n",
       "      <td>-6.557863e-01</td>\n",
       "      <td>-6.793257e-01</td>\n",
       "      <td>-6.891506e-01</td>\n",
       "      <td>-6.709773e-01</td>\n",
       "      <td>-7.435627e-01</td>\n",
       "      <td>-7.131495e-01</td>\n",
       "      <td>-6.509652e-01</td>\n",
       "      <td>-6.380641e-01</td>\n",
       "      <td>-6.394049e-01</td>\n",
       "      <td>-7.999231e-01</td>\n",
       "      <td>-7.642025e-01</td>\n",
       "      <td>-7.270112e-01</td>\n",
       "      <td>-6.678488e-01</td>\n",
       "      <td>-7.138771e-01</td>\n",
       "      <td>-6.738235e-01</td>\n",
       "      <td>-6.918580e-01</td>\n",
       "      <td>-6.788714e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.774703e-01</td>\n",
       "      <td>-2.322506e-01</td>\n",
       "      <td>-2.486515e-01</td>\n",
       "      <td>-2.120457e-01</td>\n",
       "      <td>-2.292089e-01</td>\n",
       "      <td>-2.106432e-01</td>\n",
       "      <td>-2.400524e-01</td>\n",
       "      <td>-2.672134e-01</td>\n",
       "      <td>-2.180558e-01</td>\n",
       "      <td>-1.928459e-01</td>\n",
       "      <td>-8.469964e-02</td>\n",
       "      <td>-8.381257e-03</td>\n",
       "      <td>-6.652752e-02</td>\n",
       "      <td>-9.427355e-02</td>\n",
       "      <td>-1.878739e-01</td>\n",
       "      <td>-3.179220e-01</td>\n",
       "      <td>-4.089954e-01</td>\n",
       "      <td>-3.220326e-01</td>\n",
       "      <td>-2.714467e-01</td>\n",
       "      <td>-7.841609e-02</td>\n",
       "      <td>3.359247e-02</td>\n",
       "      <td>1.591469e-01</td>\n",
       "      <td>2.112606e-01</td>\n",
       "      <td>1.083491e-01</td>\n",
       "      <td>1.869404e-01</td>\n",
       "      <td>2.308561e-01</td>\n",
       "      <td>1.772798e-01</td>\n",
       "      <td>1.600721e-01</td>\n",
       "      <td>1.615793e-01</td>\n",
       "      <td>1.190734e-01</td>\n",
       "      <td>-6.616850e-02</td>\n",
       "      <td>-4.437862e-02</td>\n",
       "      <td>-1.262995e-01</td>\n",
       "      <td>-2.262096e-01</td>\n",
       "      <td>-3.087757e-01</td>\n",
       "      <td>-2.417501e-01</td>\n",
       "      <td>-2.402772e-01</td>\n",
       "      <td>-1.268809e-01</td>\n",
       "      <td>-2.129934e-01</td>\n",
       "      <td>-1.860318e-01</td>\n",
       "      <td>-1.742944e-01</td>\n",
       "      <td>-1.972008e-01</td>\n",
       "      <td>-1.730277e-01</td>\n",
       "      <td>-2.735576e-01</td>\n",
       "      <td>-3.254731e-01</td>\n",
       "      <td>-2.939871e-01</td>\n",
       "      <td>-2.398208e-01</td>\n",
       "      <td>-2.139841e-01</td>\n",
       "      <td>-2.015434e-01</td>\n",
       "      <td>-1.851537e-01</td>\n",
       "      <td>-1.810370e-01</td>\n",
       "      <td>-2.102002e-01</td>\n",
       "      <td>-1.645716e-01</td>\n",
       "      <td>-2.252935e-01</td>\n",
       "      <td>-2.532164e-01</td>\n",
       "      <td>-2.396997e-01</td>\n",
       "      <td>-3.240352e-01</td>\n",
       "      <td>-3.329639e-01</td>\n",
       "      <td>-2.499546e-01</td>\n",
       "      <td>-2.405314e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.784345e-01</td>\n",
       "      <td>2.893335e-01</td>\n",
       "      <td>3.682681e-01</td>\n",
       "      <td>2.285353e-01</td>\n",
       "      <td>4.524231e-01</td>\n",
       "      <td>5.012417e-01</td>\n",
       "      <td>5.232608e-01</td>\n",
       "      <td>4.096773e-01</td>\n",
       "      <td>4.692723e-01</td>\n",
       "      <td>4.507410e-01</td>\n",
       "      <td>4.958032e-01</td>\n",
       "      <td>5.798756e-01</td>\n",
       "      <td>5.542820e-01</td>\n",
       "      <td>5.461209e-01</td>\n",
       "      <td>6.476456e-01</td>\n",
       "      <td>6.774890e-01</td>\n",
       "      <td>9.254848e-01</td>\n",
       "      <td>8.690373e-01</td>\n",
       "      <td>8.804084e-01</td>\n",
       "      <td>9.399196e-01</td>\n",
       "      <td>8.083857e-01</td>\n",
       "      <td>8.136570e-01</td>\n",
       "      <td>8.077787e-01</td>\n",
       "      <td>8.364219e-01</td>\n",
       "      <td>8.115900e-01</td>\n",
       "      <td>8.194722e-01</td>\n",
       "      <td>8.770920e-01</td>\n",
       "      <td>8.716615e-01</td>\n",
       "      <td>8.764117e-01</td>\n",
       "      <td>7.004289e-01</td>\n",
       "      <td>6.439769e-01</td>\n",
       "      <td>6.640521e-01</td>\n",
       "      <td>6.742455e-01</td>\n",
       "      <td>8.361697e-01</td>\n",
       "      <td>7.766817e-01</td>\n",
       "      <td>6.515635e-01</td>\n",
       "      <td>6.480175e-01</td>\n",
       "      <td>4.748773e-01</td>\n",
       "      <td>5.493604e-01</td>\n",
       "      <td>6.348106e-01</td>\n",
       "      <td>5.757088e-01</td>\n",
       "      <td>6.294876e-01</td>\n",
       "      <td>5.624103e-01</td>\n",
       "      <td>4.337440e-01</td>\n",
       "      <td>2.268742e-01</td>\n",
       "      <td>2.974485e-01</td>\n",
       "      <td>3.685825e-01</td>\n",
       "      <td>4.605360e-01</td>\n",
       "      <td>4.627081e-01</td>\n",
       "      <td>3.558478e-01</td>\n",
       "      <td>3.970293e-01</td>\n",
       "      <td>3.438640e-01</td>\n",
       "      <td>5.950106e-01</td>\n",
       "      <td>4.886751e-01</td>\n",
       "      <td>3.973675e-01</td>\n",
       "      <td>4.112618e-01</td>\n",
       "      <td>4.513169e-01</td>\n",
       "      <td>3.719959e-01</td>\n",
       "      <td>3.865486e-01</td>\n",
       "      <td>4.020352e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.706053e+00</td>\n",
       "      <td>5.944643e+00</td>\n",
       "      <td>6.836142e+00</td>\n",
       "      <td>8.025419e+00</td>\n",
       "      <td>5.878863e+00</td>\n",
       "      <td>4.710224e+00</td>\n",
       "      <td>4.074573e+00</td>\n",
       "      <td>3.816498e+00</td>\n",
       "      <td>4.274237e+00</td>\n",
       "      <td>3.746234e+00</td>\n",
       "      <td>3.763162e+00</td>\n",
       "      <td>3.261740e+00</td>\n",
       "      <td>3.127477e+00</td>\n",
       "      <td>4.268880e+00</td>\n",
       "      <td>3.317184e+00</td>\n",
       "      <td>2.672727e+00</td>\n",
       "      <td>2.220238e+00</td>\n",
       "      <td>2.099203e+00</td>\n",
       "      <td>1.924052e+00</td>\n",
       "      <td>1.667629e+00</td>\n",
       "      <td>1.519998e+00</td>\n",
       "      <td>1.471889e+00</td>\n",
       "      <td>1.414514e+00</td>\n",
       "      <td>1.372284e+00</td>\n",
       "      <td>1.328397e+00</td>\n",
       "      <td>1.268223e+00</td>\n",
       "      <td>1.215369e+00</td>\n",
       "      <td>1.293121e+00</td>\n",
       "      <td>1.493402e+00</td>\n",
       "      <td>1.902987e+00</td>\n",
       "      <td>2.160531e+00</td>\n",
       "      <td>2.310789e+00</td>\n",
       "      <td>2.828812e+00</td>\n",
       "      <td>2.433911e+00</td>\n",
       "      <td>2.349744e+00</td>\n",
       "      <td>2.334674e+00</td>\n",
       "      <td>2.448005e+00</td>\n",
       "      <td>3.108070e+00</td>\n",
       "      <td>3.322838e+00</td>\n",
       "      <td>3.470167e+00</td>\n",
       "      <td>3.574989e+00</td>\n",
       "      <td>3.245601e+00</td>\n",
       "      <td>3.798951e+00</td>\n",
       "      <td>4.227453e+00</td>\n",
       "      <td>3.346264e+00</td>\n",
       "      <td>4.255258e+00</td>\n",
       "      <td>4.954231e+00</td>\n",
       "      <td>3.894165e+00</td>\n",
       "      <td>4.075316e+00</td>\n",
       "      <td>4.553653e+00</td>\n",
       "      <td>7.039574e+00</td>\n",
       "      <td>5.980752e+00</td>\n",
       "      <td>4.016680e+00</td>\n",
       "      <td>3.330819e+00</td>\n",
       "      <td>5.008027e+00</td>\n",
       "      <td>5.448568e+00</td>\n",
       "      <td>4.795888e+00</td>\n",
       "      <td>5.585599e+00</td>\n",
       "      <td>4.615037e+00</td>\n",
       "      <td>7.450343e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1   ...            59          60\n",
       "count  2.080000e+02  2.080000e+02  ...  2.080000e+02  208.000000\n",
       "mean  -4.190024e-17  1.663333e-16  ...  3.149190e-17    0.533654\n",
       "std    1.002413e+00  1.002413e+00  ...  1.002413e+00    0.500070\n",
       "min   -1.206158e+00 -1.150725e+00  ... -1.176985e+00    0.000000\n",
       "25%   -6.894939e-01 -6.686781e-01  ... -6.788714e-01    0.000000\n",
       "50%   -2.774703e-01 -2.322506e-01  ... -2.405314e-01    1.000000\n",
       "75%    2.784345e-01  2.893335e-01  ...  4.020352e-01    1.000000\n",
       "max    4.706053e+00  5.944643e+00  ...  7.450343e+00    1.000000\n",
       "\n",
       "[8 rows x 61 columns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fd150",
   "metadata": {},
   "source": [
    "Train test val split\n",
    "80/10/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "f752c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sonar_data.drop(columns=60, axis=1)\n",
    "y = sonar_data[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "8ebe8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "bb690bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 60)\n",
      "1    91\n",
      "0    75\n",
      "Name: 60, dtype: int64\n",
      "******\n",
      "(21, 60)\n",
      "1    11\n",
      "0    10\n",
      "Name: 60, dtype: int64\n",
      "******\n",
      "(21, 60)\n",
      "0    12\n",
      "1     9\n",
      "Name: 60, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.value_counts())\n",
    "print('******')\n",
    "print(X_test.shape)\n",
    "print(y_test.value_counts())\n",
    "print('******')\n",
    "print(X_val.shape)\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc608bc5",
   "metadata": {},
   "source": [
    "Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "cdbd99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1b105",
   "metadata": {},
   "source": [
    "Vytvorenie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e4021b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nothing\n",
    "tf_model1 = Sequential()\n",
    "tf_model1.add(Dense(16, activation=tf.keras.activations.relu, input_dim=len(X_train.columns)))\n",
    "tf_model1.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model1.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model1.add(Dense(1, activation=tf.keras.activations.sigmoid))\n",
    "#dropout\n",
    "tf_model2 = Sequential()\n",
    "tf_model2.add(Dense(16, activation=tf.keras.activations.relu, input_dim=len(X_train.columns)))\n",
    "tf_model2.add(Dropout(0.5))\n",
    "tf_model2.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model2.add(Dropout(0.5))\n",
    "tf_model2.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model2.add(Dense(1, activation=tf.keras.activations.sigmoid))\n",
    "#early stopping\n",
    "tf_model3 = Sequential()\n",
    "tf_model3.add(Dense(16, activation=tf.keras.activations.relu, input_dim=len(X_train.columns)))\n",
    "tf_model3.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model3.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model3.add(Dense(1, activation=tf.keras.activations.sigmoid))\n",
    "earlyStop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "#regularization\n",
    "tf_model4 = Sequential()\n",
    "tf_model4.add(Dense(16, activation=tf.keras.activations.relu, input_dim=len(X_train.columns), kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.005, l2=0.005)))\n",
    "tf_model4.add(Dense(16, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.005, l2=0.005)))\n",
    "tf_model4.add(Dense(16, activation=tf.keras.activations.relu, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.005, l2=0.005)))\n",
    "tf_model4.add(Dense(1, activation=tf.keras.activations.sigmoid))\n",
    "#dropout + earlystop\n",
    "tf_model5 = Sequential()\n",
    "tf_model5.add(Dense(16, activation=tf.keras.activations.relu, input_dim=len(X_train.columns)))\n",
    "tf_model5.add(Dropout(0.5))\n",
    "tf_model5.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model5.add(Dropout(0.5))\n",
    "tf_model5.add(Dense(16, activation=tf.keras.activations.relu))\n",
    "tf_model5.add(Dense(1, activation=tf.keras.activations.sigmoid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "a04d45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "tf_model1.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam(), metrics=['accuracy', 'mse', precision, recall])\n",
    "tf_model2.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam(), metrics=['accuracy', 'mse', precision, recall])\n",
    "tf_model3.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam(), metrics=['accuracy', 'mse', precision, recall])\n",
    "tf_model4.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam(), metrics=['accuracy', 'mse', precision, recall])\n",
    "tf_model5.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=Adam(), metrics=['accuracy', 'mse', precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a713898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(model, name, callbackArg):\n",
    "    \n",
    "    model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=16,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val,y_val),\n",
    "    callbacks=callbackArg\n",
    "    )\n",
    "\n",
    "    model.save('./'+ name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "ce86c6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 32ms/step - loss: 0.6921 - accuracy: 0.5301 - mse: 0.2495 - precision_3: 0.5867 - recall_2: 0.4835 - val_loss: 0.6854 - val_accuracy: 0.5714 - val_mse: 0.2461 - val_precision_3: 0.5000 - val_recall_2: 1.0000\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6796 - accuracy: 0.5904 - mse: 0.2432 - precision_3: 0.5742 - recall_2: 0.9780 - val_loss: 0.6867 - val_accuracy: 0.4762 - val_mse: 0.2468 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6723 - accuracy: 0.5783 - mse: 0.2396 - precision_3: 0.5652 - recall_2: 1.0000 - val_loss: 0.6857 - val_accuracy: 0.4762 - val_mse: 0.2463 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6664 - accuracy: 0.5663 - mse: 0.2368 - precision_3: 0.5583 - recall_2: 1.0000 - val_loss: 0.6832 - val_accuracy: 0.4762 - val_mse: 0.2452 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6563 - accuracy: 0.6446 - mse: 0.2317 - precision_3: 0.6067 - recall_2: 1.0000 - val_loss: 0.6682 - val_accuracy: 0.6190 - val_mse: 0.2376 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6501 - accuracy: 0.7108 - mse: 0.2287 - precision_3: 0.6547 - recall_2: 1.0000 - val_loss: 0.6622 - val_accuracy: 0.6190 - val_mse: 0.2348 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6404 - accuracy: 0.6747 - mse: 0.2240 - precision_3: 0.6276 - recall_2: 1.0000 - val_loss: 0.6569 - val_accuracy: 0.6190 - val_mse: 0.2323 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6293 - accuracy: 0.6747 - mse: 0.2187 - precision_3: 0.6276 - recall_2: 1.0000 - val_loss: 0.6511 - val_accuracy: 0.6190 - val_mse: 0.2296 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6188 - accuracy: 0.7108 - mse: 0.2135 - precision_3: 0.6547 - recall_2: 1.0000 - val_loss: 0.6365 - val_accuracy: 0.5714 - val_mse: 0.2224 - val_precision_3: 0.5000 - val_recall_2: 0.7778\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6055 - accuracy: 0.7410 - mse: 0.2071 - precision_3: 0.6791 - recall_2: 1.0000 - val_loss: 0.6302 - val_accuracy: 0.5714 - val_mse: 0.2195 - val_precision_3: 0.5000 - val_recall_2: 0.7778\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5921 - accuracy: 0.7470 - mse: 0.2009 - precision_3: 0.6842 - recall_2: 1.0000 - val_loss: 0.6252 - val_accuracy: 0.5714 - val_mse: 0.2173 - val_precision_3: 0.5000 - val_recall_2: 0.7778\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5822 - accuracy: 0.7711 - mse: 0.1962 - precision_3: 0.7087 - recall_2: 0.9890 - val_loss: 0.6096 - val_accuracy: 0.6667 - val_mse: 0.2100 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5639 - accuracy: 0.7590 - mse: 0.1878 - precision_3: 0.6977 - recall_2: 0.9890 - val_loss: 0.6063 - val_accuracy: 0.6667 - val_mse: 0.2089 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5487 - accuracy: 0.7711 - mse: 0.1811 - precision_3: 0.7087 - recall_2: 0.9890 - val_loss: 0.5906 - val_accuracy: 0.6667 - val_mse: 0.2021 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5301 - accuracy: 0.8012 - mse: 0.1733 - precision_3: 0.7377 - recall_2: 0.9890 - val_loss: 0.5743 - val_accuracy: 0.7143 - val_mse: 0.1952 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5118 - accuracy: 0.8253 - mse: 0.1659 - precision_3: 0.8039 - recall_2: 0.9011 - val_loss: 0.5648 - val_accuracy: 0.7143 - val_mse: 0.1920 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4932 - accuracy: 0.8072 - mse: 0.1584 - precision_3: 0.7864 - recall_2: 0.8901 - val_loss: 0.5708 - val_accuracy: 0.7143 - val_mse: 0.1953 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4767 - accuracy: 0.8133 - mse: 0.1519 - precision_3: 0.7586 - recall_2: 0.9670 - val_loss: 0.5650 - val_accuracy: 0.7143 - val_mse: 0.1932 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4597 - accuracy: 0.8434 - mse: 0.1452 - precision_3: 0.7982 - recall_2: 0.9560 - val_loss: 0.5685 - val_accuracy: 0.7143 - val_mse: 0.1947 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4443 - accuracy: 0.8133 - mse: 0.1398 - precision_3: 0.8333 - recall_2: 0.8242 - val_loss: 0.5632 - val_accuracy: 0.6667 - val_mse: 0.1925 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4298 - accuracy: 0.8193 - mse: 0.1345 - precision_3: 0.8081 - recall_2: 0.8791 - val_loss: 0.5682 - val_accuracy: 0.7143 - val_mse: 0.1951 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4192 - accuracy: 0.8373 - mse: 0.1304 - precision_3: 0.8478 - recall_2: 0.8571 - val_loss: 0.5741 - val_accuracy: 0.7143 - val_mse: 0.1968 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.4089 - accuracy: 0.8554 - mse: 0.1265 - precision_3: 0.8073 - recall_2: 0.9670 - val_loss: 0.5671 - val_accuracy: 0.7143 - val_mse: 0.1930 - val_precision_3: 0.6667 - val_recall_2: 0.6667\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4091 - accuracy: 0.8434 - mse: 0.1295 - precision_3: 0.8916 - recall_2: 0.8132 - val_loss: 0.5778 - val_accuracy: 0.7143 - val_mse: 0.1990 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3947 - accuracy: 0.8494 - mse: 0.1218 - precision_3: 0.7946 - recall_2: 0.9780 - val_loss: 0.5593 - val_accuracy: 0.6667 - val_mse: 0.1913 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4120 - accuracy: 0.8253 - mse: 0.1327 - precision_3: 0.8974 - recall_2: 0.7692 - val_loss: 0.5860 - val_accuracy: 0.7143 - val_mse: 0.2018 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4357 - accuracy: 0.7651 - mse: 0.1409 - precision_3: 0.7000 - recall_2: 1.0000 - val_loss: 0.5827 - val_accuracy: 0.7143 - val_mse: 0.1992 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4233 - accuracy: 0.7771 - mse: 0.1395 - precision_3: 0.8750 - recall_2: 0.6923 - val_loss: 0.5746 - val_accuracy: 0.6667 - val_mse: 0.1950 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4110 - accuracy: 0.8072 - mse: 0.1318 - precision_3: 0.7479 - recall_2: 0.9780 - val_loss: 0.5907 - val_accuracy: 0.6667 - val_mse: 0.2018 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.3780 - accuracy: 0.8554 - mse: 0.1174 - precision_3: 0.8681 - recall_2: 0.8681 - val_loss: 0.5696 - val_accuracy: 0.6667 - val_mse: 0.1925 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3628 - accuracy: 0.8675 - mse: 0.1111 - precision_3: 0.8966 - recall_2: 0.8571 - val_loss: 0.5944 - val_accuracy: 0.7143 - val_mse: 0.2032 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3673 - accuracy: 0.8554 - mse: 0.1140 - precision_3: 0.8190 - recall_2: 0.9451 - val_loss: 0.5824 - val_accuracy: 0.6667 - val_mse: 0.1969 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3482 - accuracy: 0.8976 - mse: 0.1051 - precision_3: 0.8854 - recall_2: 0.9341 - val_loss: 0.5857 - val_accuracy: 0.7143 - val_mse: 0.1978 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3433 - accuracy: 0.8916 - mse: 0.1030 - precision_3: 0.8763 - recall_2: 0.9341 - val_loss: 0.5933 - val_accuracy: 0.6667 - val_mse: 0.1991 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3392 - accuracy: 0.8855 - mse: 0.1023 - precision_3: 0.8750 - recall_2: 0.9231 - val_loss: 0.6061 - val_accuracy: 0.6667 - val_mse: 0.2035 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3392 - accuracy: 0.8795 - mse: 0.1031 - precision_3: 0.8817 - recall_2: 0.9011 - val_loss: 0.6032 - val_accuracy: 0.7143 - val_mse: 0.2049 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3290 - accuracy: 0.8916 - mse: 0.0991 - precision_3: 0.8842 - recall_2: 0.9231 - val_loss: 0.5924 - val_accuracy: 0.6667 - val_mse: 0.1991 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.3269 - accuracy: 0.9096 - mse: 0.0977 - precision_3: 0.8958 - recall_2: 0.9451 - val_loss: 0.5939 - val_accuracy: 0.6667 - val_mse: 0.1989 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3212 - accuracy: 0.8976 - mse: 0.0962 - precision_3: 0.9022 - recall_2: 0.9121 - val_loss: 0.6062 - val_accuracy: 0.7143 - val_mse: 0.2045 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8976 - mse: 0.0989 - precision_3: 0.8776 - recall_2: 0.9451 - val_loss: 0.6035 - val_accuracy: 0.7143 - val_mse: 0.2025 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3493 - accuracy: 0.8373 - mse: 0.1086 - precision_3: 0.7857 - recall_2: 0.9670 - val_loss: 0.5996 - val_accuracy: 0.6667 - val_mse: 0.1981 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3235 - accuracy: 0.8675 - mse: 0.0986 - precision_3: 0.8966 - recall_2: 0.8571 - val_loss: 0.5902 - val_accuracy: 0.7143 - val_mse: 0.1961 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3146 - accuracy: 0.8795 - mse: 0.0951 - precision_3: 0.8381 - recall_2: 0.9670 - val_loss: 0.5792 - val_accuracy: 0.7143 - val_mse: 0.1943 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8916 - mse: 0.0964 - precision_3: 0.9195 - recall_2: 0.8791 - val_loss: 0.5921 - val_accuracy: 0.7143 - val_mse: 0.1975 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.2970 - accuracy: 0.9036 - mse: 0.0876 - precision_3: 0.8713 - recall_2: 0.9670 - val_loss: 0.6081 - val_accuracy: 0.7143 - val_mse: 0.2015 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2996 - accuracy: 0.9157 - mse: 0.0893 - precision_3: 0.9140 - recall_2: 0.9341 - val_loss: 0.6041 - val_accuracy: 0.7143 - val_mse: 0.2010 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3000 - accuracy: 0.8795 - mse: 0.0911 - precision_3: 0.8318 - recall_2: 0.9780 - val_loss: 0.5968 - val_accuracy: 0.7143 - val_mse: 0.1986 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.2907 - accuracy: 0.8976 - mse: 0.0866 - precision_3: 0.9022 - recall_2: 0.9121 - val_loss: 0.5884 - val_accuracy: 0.7143 - val_mse: 0.1956 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2818 - accuracy: 0.8976 - mse: 0.0827 - precision_3: 0.9111 - recall_2: 0.9011 - val_loss: 0.5911 - val_accuracy: 0.7143 - val_mse: 0.1953 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2771 - accuracy: 0.9036 - mse: 0.0814 - precision_3: 0.8788 - recall_2: 0.9560 - val_loss: 0.6039 - val_accuracy: 0.7143 - val_mse: 0.1981 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2736 - accuracy: 0.9096 - mse: 0.0798 - precision_3: 0.9043 - recall_2: 0.9341 - val_loss: 0.5793 - val_accuracy: 0.7143 - val_mse: 0.1914 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2636 - accuracy: 0.9217 - mse: 0.0759 - precision_3: 0.9062 - recall_2: 0.9560 - val_loss: 0.6018 - val_accuracy: 0.7143 - val_mse: 0.1978 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2642 - accuracy: 0.9096 - mse: 0.0770 - precision_3: 0.8958 - recall_2: 0.9451 - val_loss: 0.5991 - val_accuracy: 0.7143 - val_mse: 0.1969 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2579 - accuracy: 0.9157 - mse: 0.0746 - precision_3: 0.8969 - recall_2: 0.9560 - val_loss: 0.6060 - val_accuracy: 0.7143 - val_mse: 0.1958 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2530 - accuracy: 0.9157 - mse: 0.0728 - precision_3: 0.9053 - recall_2: 0.9451 - val_loss: 0.6019 - val_accuracy: 0.7143 - val_mse: 0.1955 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2535 - accuracy: 0.9096 - mse: 0.0739 - precision_3: 0.8800 - recall_2: 0.9670 - val_loss: 0.6082 - val_accuracy: 0.7143 - val_mse: 0.1971 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2550 - accuracy: 0.9157 - mse: 0.0741 - precision_3: 0.9529 - recall_2: 0.8901 - val_loss: 0.5981 - val_accuracy: 0.7143 - val_mse: 0.1949 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.8916 - mse: 0.0791 - precision_3: 0.8476 - recall_2: 0.9780 - val_loss: 0.6167 - val_accuracy: 0.7143 - val_mse: 0.1960 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2540 - accuracy: 0.9398 - mse: 0.0715 - precision_3: 0.9551 - recall_2: 0.9341 - val_loss: 0.5958 - val_accuracy: 0.7143 - val_mse: 0.1915 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2389 - accuracy: 0.9217 - mse: 0.0684 - precision_3: 0.9149 - recall_2: 0.9451 - val_loss: 0.5995 - val_accuracy: 0.7143 - val_mse: 0.1928 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.2315 - accuracy: 0.9458 - mse: 0.0647 - precision_3: 0.9362 - recall_2: 0.9670 - val_loss: 0.6086 - val_accuracy: 0.7143 - val_mse: 0.1938 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2299 - accuracy: 0.9337 - mse: 0.0650 - precision_3: 0.9255 - recall_2: 0.9560 - val_loss: 0.6084 - val_accuracy: 0.7143 - val_mse: 0.1964 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2326 - accuracy: 0.9277 - mse: 0.0653 - precision_3: 0.9647 - recall_2: 0.9011 - val_loss: 0.5982 - val_accuracy: 0.7143 - val_mse: 0.1928 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2238 - accuracy: 0.9277 - mse: 0.0638 - precision_3: 0.9072 - recall_2: 0.9670 - val_loss: 0.6100 - val_accuracy: 0.7143 - val_mse: 0.1915 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2215 - accuracy: 0.9337 - mse: 0.0620 - precision_3: 0.9651 - recall_2: 0.9121 - val_loss: 0.6092 - val_accuracy: 0.7143 - val_mse: 0.1976 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2405 - accuracy: 0.8976 - mse: 0.0714 - precision_3: 0.8700 - recall_2: 0.9560 - val_loss: 0.6143 - val_accuracy: 0.7143 - val_mse: 0.1959 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2177 - accuracy: 0.9217 - mse: 0.0630 - precision_3: 0.8980 - recall_2: 0.9670 - val_loss: 0.6430 - val_accuracy: 0.6667 - val_mse: 0.1973 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2201 - accuracy: 0.9458 - mse: 0.0618 - precision_3: 0.9659 - recall_2: 0.9341 - val_loss: 0.6268 - val_accuracy: 0.7143 - val_mse: 0.1965 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2076 - accuracy: 0.9398 - mse: 0.0582 - precision_3: 0.9175 - recall_2: 0.9780 - val_loss: 0.6228 - val_accuracy: 0.7143 - val_mse: 0.1934 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9337 - mse: 0.0541 - precision_3: 0.9444 - recall_2: 0.9341 - val_loss: 0.6060 - val_accuracy: 0.7143 - val_mse: 0.1901 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1927 - accuracy: 0.9518 - mse: 0.0511 - precision_3: 0.9560 - recall_2: 0.9560 - val_loss: 0.6067 - val_accuracy: 0.7143 - val_mse: 0.1929 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1939 - accuracy: 0.9398 - mse: 0.0529 - precision_3: 0.9355 - recall_2: 0.9560 - val_loss: 0.6286 - val_accuracy: 0.7143 - val_mse: 0.1893 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1930 - accuracy: 0.9458 - mse: 0.0520 - precision_3: 0.9457 - recall_2: 0.9560 - val_loss: 0.6300 - val_accuracy: 0.7143 - val_mse: 0.1956 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1823 - accuracy: 0.9518 - mse: 0.0487 - precision_3: 0.9462 - recall_2: 0.9670 - val_loss: 0.6357 - val_accuracy: 0.7143 - val_mse: 0.1937 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1808 - accuracy: 0.9398 - mse: 0.0481 - precision_3: 0.9451 - recall_2: 0.9451 - val_loss: 0.6445 - val_accuracy: 0.7143 - val_mse: 0.2019 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1784 - accuracy: 0.9458 - mse: 0.0475 - precision_3: 0.9457 - recall_2: 0.9560 - val_loss: 0.6329 - val_accuracy: 0.7143 - val_mse: 0.1929 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1765 - accuracy: 0.9578 - mse: 0.0476 - precision_3: 0.9468 - recall_2: 0.9780 - val_loss: 0.6721 - val_accuracy: 0.7143 - val_mse: 0.1991 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1708 - accuracy: 0.9518 - mse: 0.0452 - precision_3: 0.9560 - recall_2: 0.9560 - val_loss: 0.6439 - val_accuracy: 0.7143 - val_mse: 0.1983 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1665 - accuracy: 0.9518 - mse: 0.0440 - precision_3: 0.9368 - recall_2: 0.9780 - val_loss: 0.6599 - val_accuracy: 0.7143 - val_mse: 0.1935 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1662 - accuracy: 0.9518 - mse: 0.0434 - precision_3: 0.9560 - recall_2: 0.9560 - val_loss: 0.6462 - val_accuracy: 0.7143 - val_mse: 0.2006 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1597 - accuracy: 0.9578 - mse: 0.0411 - precision_3: 0.9667 - recall_2: 0.9560 - val_loss: 0.6337 - val_accuracy: 0.7143 - val_mse: 0.1850 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1560 - accuracy: 0.9578 - mse: 0.0398 - precision_3: 0.9565 - recall_2: 0.9670 - val_loss: 0.6471 - val_accuracy: 0.7143 - val_mse: 0.1896 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1541 - accuracy: 0.9518 - mse: 0.0400 - precision_3: 0.9560 - recall_2: 0.9560 - val_loss: 0.6761 - val_accuracy: 0.7143 - val_mse: 0.2036 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1473 - accuracy: 0.9518 - mse: 0.0383 - precision_3: 0.9462 - recall_2: 0.9670 - val_loss: 0.6807 - val_accuracy: 0.7143 - val_mse: 0.1923 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1508 - accuracy: 0.9578 - mse: 0.0385 - precision_3: 0.9565 - recall_2: 0.9670 - val_loss: 0.6798 - val_accuracy: 0.7143 - val_mse: 0.2040 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1401 - accuracy: 0.9578 - mse: 0.0353 - precision_3: 0.9468 - recall_2: 0.9780 - val_loss: 0.6974 - val_accuracy: 0.7619 - val_mse: 0.1919 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9639 - mse: 0.0363 - precision_3: 0.9775 - recall_2: 0.9560 - val_loss: 0.6921 - val_accuracy: 0.7143 - val_mse: 0.2096 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1516 - accuracy: 0.9398 - mse: 0.0426 - precision_3: 0.9175 - recall_2: 0.9780 - val_loss: 0.7178 - val_accuracy: 0.7143 - val_mse: 0.1869 - val_precision_3: 0.6667 - val_recall_2: 0.6667\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9639 - mse: 0.0396 - precision_3: 0.9670 - recall_2: 0.9670 - val_loss: 0.6951 - val_accuracy: 0.7143 - val_mse: 0.2094 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9639 - mse: 0.0368 - precision_3: 0.9670 - recall_2: 0.9670 - val_loss: 0.6941 - val_accuracy: 0.7143 - val_mse: 0.1898 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1439 - accuracy: 0.9578 - mse: 0.0394 - precision_3: 0.9468 - recall_2: 0.9780 - val_loss: 0.6986 - val_accuracy: 0.7143 - val_mse: 0.1941 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1431 - accuracy: 0.9458 - mse: 0.0392 - precision_3: 0.9881 - recall_2: 0.9121 - val_loss: 0.7202 - val_accuracy: 0.7143 - val_mse: 0.2148 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1502 - accuracy: 0.9518 - mse: 0.0427 - precision_3: 0.9368 - recall_2: 0.9780 - val_loss: 0.7300 - val_accuracy: 0.7143 - val_mse: 0.1869 - val_precision_3: 0.6667 - val_recall_2: 0.6667\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1370 - accuracy: 0.9759 - mse: 0.0350 - precision_3: 0.9780 - recall_2: 0.9780 - val_loss: 0.6973 - val_accuracy: 0.7143 - val_mse: 0.2032 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1202 - accuracy: 0.9639 - mse: 0.0302 - precision_3: 0.9570 - recall_2: 0.9780 - val_loss: 0.7084 - val_accuracy: 0.7619 - val_mse: 0.1935 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 0.9699 - mse: 0.0296 - precision_3: 0.9778 - recall_2: 0.9670 - val_loss: 0.7324 - val_accuracy: 0.7143 - val_mse: 0.2039 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1139 - accuracy: 0.9639 - mse: 0.0275 - precision_3: 0.9670 - recall_2: 0.9670 - val_loss: 0.7469 - val_accuracy: 0.7619 - val_mse: 0.1977 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1147 - accuracy: 0.9699 - mse: 0.0275 - precision_3: 0.9778 - recall_2: 0.9670 - val_loss: 0.7360 - val_accuracy: 0.7143 - val_mse: 0.2055 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.9699 - mse: 0.0266 - precision_3: 0.9674 - recall_2: 0.9780 - val_loss: 0.7397 - val_accuracy: 0.7143 - val_mse: 0.1976 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9819 - mse: 0.0245 - precision_3: 0.9889 - recall_2: 0.9780 - val_loss: 0.7570 - val_accuracy: 0.7143 - val_mse: 0.2095 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model1\\assets\n"
     ]
    }
   ],
   "source": [
    "fitModel(tf_model1, 'model1', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1723e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 32ms/step - loss: 0.6902 - accuracy: 0.5361 - mse: 0.2480 - precision_3: 0.6349 - recall_2: 0.4000 - val_loss: 0.6838 - val_accuracy: 0.5714 - val_mse: 0.2454 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7047 - accuracy: 0.5181 - mse: 0.2557 - precision_3: 0.5696 - recall_2: 0.4945 - val_loss: 0.6925 - val_accuracy: 0.4762 - val_mse: 0.2497 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7256 - accuracy: 0.4819 - mse: 0.2655 - precision_3: 0.5238 - recall_2: 0.6044 - val_loss: 0.6936 - val_accuracy: 0.4286 - val_mse: 0.2503 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6847 - accuracy: 0.5723 - mse: 0.2458 - precision_3: 0.5943 - recall_2: 0.6923 - val_loss: 0.6946 - val_accuracy: 0.4286 - val_mse: 0.2508 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6744 - accuracy: 0.5301 - mse: 0.2408 - precision_3: 0.5596 - recall_2: 0.6703 - val_loss: 0.6903 - val_accuracy: 0.5238 - val_mse: 0.2486 - val_precision_3: 0.4737 - val_recall_2: 1.0000\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5602 - mse: 0.2495 - precision_3: 0.5750 - recall_2: 0.7582 - val_loss: 0.6872 - val_accuracy: 0.5238 - val_mse: 0.2471 - val_precision_3: 0.4737 - val_recall_2: 1.0000\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6820 - accuracy: 0.5482 - mse: 0.2445 - precision_3: 0.5727 - recall_2: 0.6923 - val_loss: 0.6880 - val_accuracy: 0.4286 - val_mse: 0.2474 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7024 - accuracy: 0.5301 - mse: 0.2543 - precision_3: 0.5607 - recall_2: 0.6593 - val_loss: 0.6898 - val_accuracy: 0.4286 - val_mse: 0.2484 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7016 - accuracy: 0.5301 - mse: 0.2541 - precision_3: 0.5489 - recall_2: 0.8022 - val_loss: 0.6889 - val_accuracy: 0.4286 - val_mse: 0.2479 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6804 - accuracy: 0.5843 - mse: 0.2438 - precision_3: 0.5932 - recall_2: 0.7692 - val_loss: 0.6803 - val_accuracy: 0.5714 - val_mse: 0.2436 - val_precision_3: 0.5000 - val_recall_2: 1.0000\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7018 - accuracy: 0.5060 - mse: 0.2542 - precision_3: 0.5378 - recall_2: 0.7033 - val_loss: 0.6767 - val_accuracy: 0.5714 - val_mse: 0.2418 - val_precision_3: 0.5000 - val_recall_2: 1.0000\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6741 - accuracy: 0.6145 - mse: 0.2406 - precision_3: 0.6116 - recall_2: 0.8132 - val_loss: 0.6785 - val_accuracy: 0.5714 - val_mse: 0.2427 - val_precision_3: 0.5000 - val_recall_2: 1.0000\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.5663 - mse: 0.2438 - precision_3: 0.5826 - recall_2: 0.7363 - val_loss: 0.6774 - val_accuracy: 0.6190 - val_mse: 0.2422 - val_precision_3: 0.5294 - val_recall_2: 1.0000\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6805 - accuracy: 0.5843 - mse: 0.2438 - precision_3: 0.5917 - recall_2: 0.7802 - val_loss: 0.6706 - val_accuracy: 0.7143 - val_mse: 0.2388 - val_precision_3: 0.6000 - val_recall_2: 1.0000\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6717 - accuracy: 0.5663 - mse: 0.2396 - precision_3: 0.5856 - recall_2: 0.7143 - val_loss: 0.6667 - val_accuracy: 0.7619 - val_mse: 0.2368 - val_precision_3: 0.6429 - val_recall_2: 1.0000\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6765 - accuracy: 0.5843 - mse: 0.2419 - precision_3: 0.5965 - recall_2: 0.7473 - val_loss: 0.6649 - val_accuracy: 0.7619 - val_mse: 0.2359 - val_precision_3: 0.6429 - val_recall_2: 1.0000\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6721 - accuracy: 0.5843 - mse: 0.2398 - precision_3: 0.5982 - recall_2: 0.7363 - val_loss: 0.6612 - val_accuracy: 0.7143 - val_mse: 0.2341 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6551 - accuracy: 0.5843 - mse: 0.2317 - precision_3: 0.5965 - recall_2: 0.7473 - val_loss: 0.6603 - val_accuracy: 0.7143 - val_mse: 0.2337 - val_precision_3: 0.6000 - val_recall_2: 1.0000\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6544 - accuracy: 0.6205 - mse: 0.2312 - precision_3: 0.6111 - recall_2: 0.8462 - val_loss: 0.6593 - val_accuracy: 0.6667 - val_mse: 0.2332 - val_precision_3: 0.5625 - val_recall_2: 1.0000\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6510 - accuracy: 0.6386 - mse: 0.2297 - precision_3: 0.6281 - recall_2: 0.8352 - val_loss: 0.6597 - val_accuracy: 0.6667 - val_mse: 0.2334 - val_precision_3: 0.5625 - val_recall_2: 1.0000\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6470 - accuracy: 0.6627 - mse: 0.2278 - precision_3: 0.6522 - recall_2: 0.8242 - val_loss: 0.6502 - val_accuracy: 0.7143 - val_mse: 0.2287 - val_precision_3: 0.6000 - val_recall_2: 1.0000\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6660 - accuracy: 0.6084 - mse: 0.2371 - precision_3: 0.6182 - recall_2: 0.7473 - val_loss: 0.6391 - val_accuracy: 0.6667 - val_mse: 0.2232 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6768 - accuracy: 0.6024 - mse: 0.2425 - precision_3: 0.6126 - recall_2: 0.7473 - val_loss: 0.6420 - val_accuracy: 0.6667 - val_mse: 0.2247 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6483 - accuracy: 0.6325 - mse: 0.2284 - precision_3: 0.6339 - recall_2: 0.7802 - val_loss: 0.6435 - val_accuracy: 0.6667 - val_mse: 0.2254 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6557 - accuracy: 0.6386 - mse: 0.2322 - precision_3: 0.6449 - recall_2: 0.7582 - val_loss: 0.6342 - val_accuracy: 0.6667 - val_mse: 0.2208 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6553 - accuracy: 0.6084 - mse: 0.2319 - precision_3: 0.6226 - recall_2: 0.7253 - val_loss: 0.6374 - val_accuracy: 0.6190 - val_mse: 0.2224 - val_precision_3: 0.5385 - val_recall_2: 0.7778\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6506 - accuracy: 0.6145 - mse: 0.2299 - precision_3: 0.6098 - recall_2: 0.8242 - val_loss: 0.6433 - val_accuracy: 0.7143 - val_mse: 0.2253 - val_precision_3: 0.6000 - val_recall_2: 1.0000\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.6747 - mse: 0.2232 - precision_3: 0.6529 - recall_2: 0.8681 - val_loss: 0.6535 - val_accuracy: 0.7143 - val_mse: 0.2304 - val_precision_3: 0.6000 - val_recall_2: 1.0000\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6457 - accuracy: 0.6205 - mse: 0.2275 - precision_3: 0.6167 - recall_2: 0.8132 - val_loss: 0.6539 - val_accuracy: 0.6667 - val_mse: 0.2306 - val_precision_3: 0.5625 - val_recall_2: 1.0000\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6370 - accuracy: 0.6506 - mse: 0.2233 - precision_3: 0.6341 - recall_2: 0.8571 - val_loss: 0.6549 - val_accuracy: 0.6667 - val_mse: 0.2312 - val_precision_3: 0.5625 - val_recall_2: 1.0000\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6395 - accuracy: 0.6627 - mse: 0.2251 - precision_3: 0.6423 - recall_2: 0.8681 - val_loss: 0.6318 - val_accuracy: 0.7143 - val_mse: 0.2197 - val_precision_3: 0.6000 - val_recall_2: 1.0000\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6238 - accuracy: 0.6687 - mse: 0.2176 - precision_3: 0.6579 - recall_2: 0.8242 - val_loss: 0.6263 - val_accuracy: 0.7619 - val_mse: 0.2171 - val_precision_3: 0.6429 - val_recall_2: 1.0000\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6456 - accuracy: 0.6084 - mse: 0.2273 - precision_3: 0.6121 - recall_2: 0.7802 - val_loss: 0.6281 - val_accuracy: 0.7143 - val_mse: 0.2180 - val_precision_3: 0.6000 - val_recall_2: 1.0000\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6159 - accuracy: 0.6627 - mse: 0.2143 - precision_3: 0.6496 - recall_2: 0.8352 - val_loss: 0.6190 - val_accuracy: 0.7619 - val_mse: 0.2136 - val_precision_3: 0.6429 - val_recall_2: 1.0000\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6097 - accuracy: 0.6867 - mse: 0.2107 - precision_3: 0.6857 - recall_2: 0.7912 - val_loss: 0.6044 - val_accuracy: 0.7143 - val_mse: 0.2067 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.6024 - mse: 0.2355 - precision_3: 0.6289 - recall_2: 0.6703 - val_loss: 0.6131 - val_accuracy: 0.7143 - val_mse: 0.2109 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5987 - accuracy: 0.6807 - mse: 0.2063 - precision_3: 0.6696 - recall_2: 0.8242 - val_loss: 0.6206 - val_accuracy: 0.7619 - val_mse: 0.2145 - val_precision_3: 0.6429 - val_recall_2: 1.0000\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6309 - accuracy: 0.6687 - mse: 0.2210 - precision_3: 0.6500 - recall_2: 0.8571 - val_loss: 0.6109 - val_accuracy: 0.7619 - val_mse: 0.2097 - val_precision_3: 0.6429 - val_recall_2: 1.0000\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 0.6325 - mse: 0.2269 - precision_3: 0.6442 - recall_2: 0.7363 - val_loss: 0.6043 - val_accuracy: 0.8095 - val_mse: 0.2066 - val_precision_3: 0.6923 - val_recall_2: 1.0000\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5933 - accuracy: 0.7048 - mse: 0.2022 - precision_3: 0.6944 - recall_2: 0.8242 - val_loss: 0.6108 - val_accuracy: 0.7143 - val_mse: 0.2099 - val_precision_3: 0.6000 - val_recall_2: 1.0000\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6091 - accuracy: 0.6627 - mse: 0.2108 - precision_3: 0.6549 - recall_2: 0.8132 - val_loss: 0.5981 - val_accuracy: 0.7619 - val_mse: 0.2039 - val_precision_3: 0.6429 - val_recall_2: 1.0000\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5956 - accuracy: 0.7048 - mse: 0.2052 - precision_3: 0.7019 - recall_2: 0.8022 - val_loss: 0.5919 - val_accuracy: 0.7143 - val_mse: 0.2010 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6074 - accuracy: 0.6807 - mse: 0.2105 - precision_3: 0.6696 - recall_2: 0.8242 - val_loss: 0.5917 - val_accuracy: 0.7619 - val_mse: 0.2010 - val_precision_3: 0.6429 - val_recall_2: 1.0000\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5703 - accuracy: 0.7169 - mse: 0.1929 - precision_3: 0.7075 - recall_2: 0.8242 - val_loss: 0.5790 - val_accuracy: 0.7619 - val_mse: 0.1954 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5645 - accuracy: 0.7169 - mse: 0.1909 - precision_3: 0.7157 - recall_2: 0.8022 - val_loss: 0.5728 - val_accuracy: 0.7619 - val_mse: 0.1928 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5739 - accuracy: 0.7410 - mse: 0.1954 - precision_3: 0.7222 - recall_2: 0.8571 - val_loss: 0.5594 - val_accuracy: 0.7619 - val_mse: 0.1870 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5878 - accuracy: 0.7349 - mse: 0.2013 - precision_3: 0.7238 - recall_2: 0.8352 - val_loss: 0.5647 - val_accuracy: 0.7619 - val_mse: 0.1896 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5316 - accuracy: 0.7711 - mse: 0.1769 - precision_3: 0.7477 - recall_2: 0.8791 - val_loss: 0.5665 - val_accuracy: 0.7619 - val_mse: 0.1904 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5373 - accuracy: 0.7530 - mse: 0.1796 - precision_3: 0.7500 - recall_2: 0.8242 - val_loss: 0.5565 - val_accuracy: 0.7619 - val_mse: 0.1863 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5327 - accuracy: 0.7590 - mse: 0.1773 - precision_3: 0.7476 - recall_2: 0.8462 - val_loss: 0.5455 - val_accuracy: 0.7619 - val_mse: 0.1825 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4985 - accuracy: 0.7892 - mse: 0.1606 - precision_3: 0.8043 - recall_2: 0.8132 - val_loss: 0.5350 - val_accuracy: 0.7619 - val_mse: 0.1789 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5279 - accuracy: 0.7289 - mse: 0.1775 - precision_3: 0.7170 - recall_2: 0.8352 - val_loss: 0.5399 - val_accuracy: 0.7619 - val_mse: 0.1811 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4974 - accuracy: 0.7711 - mse: 0.1653 - precision_3: 0.7624 - recall_2: 0.8462 - val_loss: 0.5370 - val_accuracy: 0.7619 - val_mse: 0.1805 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4887 - accuracy: 0.8072 - mse: 0.1595 - precision_3: 0.7980 - recall_2: 0.8681 - val_loss: 0.5365 - val_accuracy: 0.7619 - val_mse: 0.1805 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5479 - accuracy: 0.7289 - mse: 0.1863 - precision_3: 0.7447 - recall_2: 0.7692 - val_loss: 0.5335 - val_accuracy: 0.7619 - val_mse: 0.1794 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5393 - accuracy: 0.7169 - mse: 0.1838 - precision_3: 0.6964 - recall_2: 0.8571 - val_loss: 0.5497 - val_accuracy: 0.7143 - val_mse: 0.1859 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4689 - accuracy: 0.7892 - mse: 0.1525 - precision_3: 0.7456 - recall_2: 0.9341 - val_loss: 0.5571 - val_accuracy: 0.7143 - val_mse: 0.1895 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4726 - accuracy: 0.8072 - mse: 0.1528 - precision_3: 0.7864 - recall_2: 0.8901 - val_loss: 0.5502 - val_accuracy: 0.7143 - val_mse: 0.1885 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5184 - accuracy: 0.7651 - mse: 0.1713 - precision_3: 0.7708 - recall_2: 0.8132 - val_loss: 0.5523 - val_accuracy: 0.7143 - val_mse: 0.1894 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5147 - accuracy: 0.7530 - mse: 0.1705 - precision_3: 0.7604 - recall_2: 0.8022 - val_loss: 0.5534 - val_accuracy: 0.6667 - val_mse: 0.1898 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4866 - accuracy: 0.8072 - mse: 0.1558 - precision_3: 0.7658 - recall_2: 0.9341 - val_loss: 0.5480 - val_accuracy: 0.7619 - val_mse: 0.1863 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4582 - accuracy: 0.8072 - mse: 0.1490 - precision_3: 0.7864 - recall_2: 0.8901 - val_loss: 0.5394 - val_accuracy: 0.7143 - val_mse: 0.1838 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4855 - accuracy: 0.8193 - mse: 0.1579 - precision_3: 0.8081 - recall_2: 0.8791 - val_loss: 0.5419 - val_accuracy: 0.7619 - val_mse: 0.1852 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5002 - accuracy: 0.8133 - mse: 0.1643 - precision_3: 0.7941 - recall_2: 0.8901 - val_loss: 0.5628 - val_accuracy: 0.7143 - val_mse: 0.1933 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4931 - accuracy: 0.7590 - mse: 0.1629 - precision_3: 0.7179 - recall_2: 0.9231 - val_loss: 0.5708 - val_accuracy: 0.7143 - val_mse: 0.1966 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4731 - accuracy: 0.7952 - mse: 0.1525 - precision_3: 0.7767 - recall_2: 0.8791 - val_loss: 0.5736 - val_accuracy: 0.6667 - val_mse: 0.1989 - val_precision_3: 0.6000 - val_recall_2: 0.6667\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5547 - accuracy: 0.7289 - mse: 0.1813 - precision_3: 0.7500 - recall_2: 0.7582 - val_loss: 0.5604 - val_accuracy: 0.7143 - val_mse: 0.1929 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4608 - accuracy: 0.8072 - mse: 0.1469 - precision_3: 0.7611 - recall_2: 0.9451 - val_loss: 0.5666 - val_accuracy: 0.7143 - val_mse: 0.1946 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4782 - accuracy: 0.7651 - mse: 0.1559 - precision_3: 0.7241 - recall_2: 0.9231 - val_loss: 0.5470 - val_accuracy: 0.7143 - val_mse: 0.1880 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4611 - accuracy: 0.8072 - mse: 0.1511 - precision_3: 0.7810 - recall_2: 0.9011 - val_loss: 0.5615 - val_accuracy: 0.7143 - val_mse: 0.1940 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4626 - accuracy: 0.8072 - mse: 0.1492 - precision_3: 0.8105 - recall_2: 0.8462 - val_loss: 0.5542 - val_accuracy: 0.7143 - val_mse: 0.1919 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4493 - accuracy: 0.8012 - mse: 0.1477 - precision_3: 0.7788 - recall_2: 0.8901 - val_loss: 0.5590 - val_accuracy: 0.6667 - val_mse: 0.1936 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4083 - accuracy: 0.8253 - mse: 0.1283 - precision_3: 0.7818 - recall_2: 0.9451 - val_loss: 0.5586 - val_accuracy: 0.7143 - val_mse: 0.1937 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4649 - accuracy: 0.8072 - mse: 0.1513 - precision_3: 0.8172 - recall_2: 0.8352 - val_loss: 0.5659 - val_accuracy: 0.7143 - val_mse: 0.1978 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4940 - accuracy: 0.7651 - mse: 0.1637 - precision_3: 0.7203 - recall_2: 0.9341 - val_loss: 0.5904 - val_accuracy: 0.6667 - val_mse: 0.2085 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5056 - accuracy: 0.7771 - mse: 0.1673 - precision_3: 0.7700 - recall_2: 0.8462 - val_loss: 0.5808 - val_accuracy: 0.6667 - val_mse: 0.2036 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4467 - accuracy: 0.8193 - mse: 0.1415 - precision_3: 0.7748 - recall_2: 0.9451 - val_loss: 0.5662 - val_accuracy: 0.7143 - val_mse: 0.1965 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4392 - accuracy: 0.8253 - mse: 0.1400 - precision_3: 0.7870 - recall_2: 0.9341 - val_loss: 0.5738 - val_accuracy: 0.7143 - val_mse: 0.2004 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.8313 - mse: 0.1377 - precision_3: 0.8182 - recall_2: 0.8901 - val_loss: 0.5770 - val_accuracy: 0.7143 - val_mse: 0.2028 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3986 - accuracy: 0.8253 - mse: 0.1258 - precision_3: 0.7870 - recall_2: 0.9341 - val_loss: 0.5814 - val_accuracy: 0.6667 - val_mse: 0.2050 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4108 - accuracy: 0.8193 - mse: 0.1302 - precision_3: 0.7699 - recall_2: 0.9560 - val_loss: 0.5897 - val_accuracy: 0.7143 - val_mse: 0.2057 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.7831 - mse: 0.1461 - precision_3: 0.7723 - recall_2: 0.8571 - val_loss: 0.5789 - val_accuracy: 0.7143 - val_mse: 0.2011 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4027 - accuracy: 0.8253 - mse: 0.1267 - precision_3: 0.8163 - recall_2: 0.8791 - val_loss: 0.5739 - val_accuracy: 0.7143 - val_mse: 0.1983 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4329 - accuracy: 0.8193 - mse: 0.1404 - precision_3: 0.7905 - recall_2: 0.9121 - val_loss: 0.5754 - val_accuracy: 0.7143 - val_mse: 0.1991 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3971 - accuracy: 0.8193 - mse: 0.1259 - precision_3: 0.8280 - recall_2: 0.8462 - val_loss: 0.5874 - val_accuracy: 0.7143 - val_mse: 0.2053 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4032 - accuracy: 0.8373 - mse: 0.1260 - precision_3: 0.8077 - recall_2: 0.9231 - val_loss: 0.5956 - val_accuracy: 0.7143 - val_mse: 0.2078 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4152 - accuracy: 0.8193 - mse: 0.1338 - precision_3: 0.8081 - recall_2: 0.8791 - val_loss: 0.6050 - val_accuracy: 0.6667 - val_mse: 0.2131 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3841 - accuracy: 0.8554 - mse: 0.1191 - precision_3: 0.8384 - recall_2: 0.9121 - val_loss: 0.6087 - val_accuracy: 0.7143 - val_mse: 0.2103 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3509 - accuracy: 0.8675 - mse: 0.1081 - precision_3: 0.8710 - recall_2: 0.8901 - val_loss: 0.6045 - val_accuracy: 0.7143 - val_mse: 0.2095 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4197 - accuracy: 0.7892 - mse: 0.1365 - precision_3: 0.7593 - recall_2: 0.9011 - val_loss: 0.6022 - val_accuracy: 0.7143 - val_mse: 0.2084 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3394 - accuracy: 0.8735 - mse: 0.1020 - precision_3: 0.8431 - recall_2: 0.9451 - val_loss: 0.6357 - val_accuracy: 0.6667 - val_mse: 0.2223 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4162 - accuracy: 0.8373 - mse: 0.1294 - precision_3: 0.8137 - recall_2: 0.9121 - val_loss: 0.6234 - val_accuracy: 0.6667 - val_mse: 0.2209 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4464 - accuracy: 0.7711 - mse: 0.1451 - precision_3: 0.7304 - recall_2: 0.9231 - val_loss: 0.5919 - val_accuracy: 0.6667 - val_mse: 0.2105 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3950 - accuracy: 0.8614 - mse: 0.1211 - precision_3: 0.8400 - recall_2: 0.9231 - val_loss: 0.5919 - val_accuracy: 0.7143 - val_mse: 0.2080 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3708 - accuracy: 0.8554 - mse: 0.1153 - precision_3: 0.8454 - recall_2: 0.9011 - val_loss: 0.6113 - val_accuracy: 0.7143 - val_mse: 0.2123 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3614 - accuracy: 0.8434 - mse: 0.1134 - precision_3: 0.8095 - recall_2: 0.9341 - val_loss: 0.5907 - val_accuracy: 0.7143 - val_mse: 0.2080 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8855 - mse: 0.0994 - precision_3: 0.8529 - recall_2: 0.9560 - val_loss: 0.5853 - val_accuracy: 0.7143 - val_mse: 0.2043 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3629 - accuracy: 0.8494 - mse: 0.1155 - precision_3: 0.8367 - recall_2: 0.9011 - val_loss: 0.5844 - val_accuracy: 0.7143 - val_mse: 0.2044 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3950 - accuracy: 0.8313 - mse: 0.1266 - precision_3: 0.7788 - recall_2: 0.9670 - val_loss: 0.6027 - val_accuracy: 0.6667 - val_mse: 0.2120 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3478 - accuracy: 0.8675 - mse: 0.1075 - precision_3: 0.8224 - recall_2: 0.9670 - val_loss: 0.6169 - val_accuracy: 0.7143 - val_mse: 0.2155 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model2\\assets\n"
     ]
    }
   ],
   "source": [
    "fitModel(tf_model2, 'model2', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "92b0e13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 34ms/step - loss: 0.6980 - accuracy: 0.4940 - mse: 0.2524 - precision_3: 0.5806 - recall_2: 0.3600 - val_loss: 0.6974 - val_accuracy: 0.5238 - val_mse: 0.2521 - val_precision_3: 0.4444 - val_recall_2: 0.4444\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6868 - accuracy: 0.5181 - mse: 0.2468 - precision_3: 0.5514 - recall_2: 0.6484 - val_loss: 0.6955 - val_accuracy: 0.5238 - val_mse: 0.2512 - val_precision_3: 0.4615 - val_recall_2: 0.6667\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6787 - accuracy: 0.6265 - mse: 0.2428 - precision_3: 0.6124 - recall_2: 0.8681 - val_loss: 0.6930 - val_accuracy: 0.5238 - val_mse: 0.2499 - val_precision_3: 0.4615 - val_recall_2: 0.6667\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6719 - accuracy: 0.6566 - mse: 0.2394 - precision_3: 0.6181 - recall_2: 0.9780 - val_loss: 0.6874 - val_accuracy: 0.5714 - val_mse: 0.2471 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.7048 - mse: 0.2361 - precision_3: 0.6522 - recall_2: 0.9890 - val_loss: 0.6799 - val_accuracy: 0.5714 - val_mse: 0.2434 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6548 - accuracy: 0.6988 - mse: 0.2309 - precision_3: 0.6475 - recall_2: 0.9890 - val_loss: 0.6783 - val_accuracy: 0.6190 - val_mse: 0.2426 - val_precision_3: 0.5294 - val_recall_2: 1.0000\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6418 - accuracy: 0.7349 - mse: 0.2245 - precision_3: 0.6741 - recall_2: 1.0000 - val_loss: 0.6683 - val_accuracy: 0.6667 - val_mse: 0.2377 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6279 - accuracy: 0.7470 - mse: 0.2178 - precision_3: 0.6870 - recall_2: 0.9890 - val_loss: 0.6620 - val_accuracy: 0.6190 - val_mse: 0.2347 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6107 - accuracy: 0.7470 - mse: 0.2096 - precision_3: 0.6960 - recall_2: 0.9560 - val_loss: 0.6502 - val_accuracy: 0.6667 - val_mse: 0.2291 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5992 - accuracy: 0.7169 - mse: 0.2047 - precision_3: 0.6618 - recall_2: 0.9890 - val_loss: 0.6405 - val_accuracy: 0.6190 - val_mse: 0.2248 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5879 - accuracy: 0.7831 - mse: 0.1990 - precision_3: 0.7957 - recall_2: 0.8132 - val_loss: 0.6194 - val_accuracy: 0.7143 - val_mse: 0.2147 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5555 - accuracy: 0.7892 - mse: 0.1843 - precision_3: 0.7373 - recall_2: 0.9560 - val_loss: 0.6449 - val_accuracy: 0.5714 - val_mse: 0.2280 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5534 - accuracy: 0.7470 - mse: 0.1849 - precision_3: 0.6870 - recall_2: 0.9890 - val_loss: 0.6143 - val_accuracy: 0.7143 - val_mse: 0.2138 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5369 - accuracy: 0.7892 - mse: 0.1762 - precision_3: 0.8043 - recall_2: 0.8132 - val_loss: 0.5965 - val_accuracy: 0.6667 - val_mse: 0.2057 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5158 - accuracy: 0.7771 - mse: 0.1677 - precision_3: 0.7547 - recall_2: 0.8791 - val_loss: 0.6210 - val_accuracy: 0.6667 - val_mse: 0.2181 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4962 - accuracy: 0.8012 - mse: 0.1593 - precision_3: 0.7959 - recall_2: 0.8571 - val_loss: 0.5879 - val_accuracy: 0.6667 - val_mse: 0.2037 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4878 - accuracy: 0.8072 - mse: 0.1567 - precision_3: 0.7706 - recall_2: 0.9231 - val_loss: 0.5948 - val_accuracy: 0.7143 - val_mse: 0.2081 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4809 - accuracy: 0.7952 - mse: 0.1539 - precision_3: 0.8202 - recall_2: 0.8022 - val_loss: 0.5931 - val_accuracy: 0.6667 - val_mse: 0.2078 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4476 - accuracy: 0.8193 - mse: 0.1405 - precision_3: 0.7905 - recall_2: 0.9121 - val_loss: 0.5985 - val_accuracy: 0.6667 - val_mse: 0.2104 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.8313 - mse: 0.1360 - precision_3: 0.8119 - recall_2: 0.9011 - val_loss: 0.5811 - val_accuracy: 0.6667 - val_mse: 0.2040 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4242 - accuracy: 0.8313 - mse: 0.1327 - precision_3: 0.8316 - recall_2: 0.8681 - val_loss: 0.6067 - val_accuracy: 0.6667 - val_mse: 0.2142 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4083 - accuracy: 0.8193 - mse: 0.1273 - precision_3: 0.7961 - recall_2: 0.9011 - val_loss: 0.5852 - val_accuracy: 0.6190 - val_mse: 0.2072 - val_precision_3: 0.5455 - val_recall_2: 0.6667\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3961 - accuracy: 0.8373 - mse: 0.1225 - precision_3: 0.8333 - recall_2: 0.8791 - val_loss: 0.6255 - val_accuracy: 0.6667 - val_mse: 0.2202 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3809 - accuracy: 0.8614 - mse: 0.1175 - precision_3: 0.8542 - recall_2: 0.9011 - val_loss: 0.6022 - val_accuracy: 0.6667 - val_mse: 0.2123 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.4057 - accuracy: 0.8012 - mse: 0.1297 - precision_3: 0.7417 - recall_2: 0.9780 - val_loss: 0.6006 - val_accuracy: 0.6667 - val_mse: 0.2112 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3919 - accuracy: 0.8133 - mse: 0.1248 - precision_3: 0.9054 - recall_2: 0.7363 - val_loss: 0.6017 - val_accuracy: 0.6667 - val_mse: 0.2119 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3619 - accuracy: 0.8373 - mse: 0.1111 - precision_3: 0.7909 - recall_2: 0.9560 - val_loss: 0.6100 - val_accuracy: 0.6667 - val_mse: 0.2150 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3550 - accuracy: 0.8554 - mse: 0.1093 - precision_3: 0.8941 - recall_2: 0.8352 - val_loss: 0.6103 - val_accuracy: 0.6667 - val_mse: 0.2139 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3636 - accuracy: 0.8313 - mse: 0.1144 - precision_3: 0.7739 - recall_2: 0.9780 - val_loss: 0.6107 - val_accuracy: 0.6667 - val_mse: 0.2140 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.3367 - accuracy: 0.8855 - mse: 0.1025 - precision_3: 0.9091 - recall_2: 0.8791 - val_loss: 0.6012 - val_accuracy: 0.6667 - val_mse: 0.2100 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model3\\assets\n"
     ]
    }
   ],
   "source": [
    "fitModel(tf_model3, 'model3', [earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ad287c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 34ms/step - loss: 2.1846 - accuracy: 0.5542 - mse: 0.2469 - precision_3: 0.5543 - recall_2: 0.9700 - val_loss: 2.1577 - val_accuracy: 0.4286 - val_mse: 0.2587 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0902 - accuracy: 0.5482 - mse: 0.2440 - precision_3: 0.5482 - recall_2: 1.0000 - val_loss: 2.0699 - val_accuracy: 0.4286 - val_mse: 0.2586 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2.0007 - accuracy: 0.5482 - mse: 0.2423 - precision_3: 0.5482 - recall_2: 1.0000 - val_loss: 1.9877 - val_accuracy: 0.4286 - val_mse: 0.2594 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9146 - accuracy: 0.5482 - mse: 0.2402 - precision_3: 0.5482 - recall_2: 1.0000 - val_loss: 1.9114 - val_accuracy: 0.4286 - val_mse: 0.2607 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.8331 - accuracy: 0.5542 - mse: 0.2378 - precision_3: 0.5515 - recall_2: 1.0000 - val_loss: 1.8333 - val_accuracy: 0.4286 - val_mse: 0.2586 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.7574 - accuracy: 0.5723 - mse: 0.2362 - precision_3: 0.5617 - recall_2: 1.0000 - val_loss: 1.7599 - val_accuracy: 0.4286 - val_mse: 0.2570 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.6861 - accuracy: 0.5723 - mse: 0.2346 - precision_3: 0.5617 - recall_2: 1.0000 - val_loss: 1.6892 - val_accuracy: 0.4286 - val_mse: 0.2548 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.6186 - accuracy: 0.5964 - mse: 0.2332 - precision_3: 0.5759 - recall_2: 1.0000 - val_loss: 1.6245 - val_accuracy: 0.4286 - val_mse: 0.2536 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.5555 - accuracy: 0.5904 - mse: 0.2323 - precision_3: 0.5723 - recall_2: 1.0000 - val_loss: 1.5633 - val_accuracy: 0.4286 - val_mse: 0.2529 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4965 - accuracy: 0.6265 - mse: 0.2321 - precision_3: 0.5948 - recall_2: 1.0000 - val_loss: 1.4989 - val_accuracy: 0.4762 - val_mse: 0.2492 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.4392 - accuracy: 0.5843 - mse: 0.2311 - precision_3: 0.5688 - recall_2: 1.0000 - val_loss: 1.4492 - val_accuracy: 0.4286 - val_mse: 0.2511 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3838 - accuracy: 0.6024 - mse: 0.2295 - precision_3: 0.5796 - recall_2: 1.0000 - val_loss: 1.3928 - val_accuracy: 0.4762 - val_mse: 0.2481 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.3335 - accuracy: 0.6325 - mse: 0.2287 - precision_3: 0.5987 - recall_2: 1.0000 - val_loss: 1.3436 - val_accuracy: 0.4762 - val_mse: 0.2468 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2867 - accuracy: 0.6566 - mse: 0.2280 - precision_3: 0.6149 - recall_2: 1.0000 - val_loss: 1.2979 - val_accuracy: 0.4762 - val_mse: 0.2459 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.2436 - accuracy: 0.6084 - mse: 0.2277 - precision_3: 0.5833 - recall_2: 1.0000 - val_loss: 1.2616 - val_accuracy: 0.4762 - val_mse: 0.2482 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.2002 - accuracy: 0.5904 - mse: 0.2261 - precision_3: 0.5723 - recall_2: 1.0000 - val_loss: 1.2198 - val_accuracy: 0.4762 - val_mse: 0.2465 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.1608 - accuracy: 0.6205 - mse: 0.2250 - precision_3: 0.5909 - recall_2: 1.0000 - val_loss: 1.1822 - val_accuracy: 0.4762 - val_mse: 0.2456 - val_precision_3: 0.4500 - val_recall_2: 1.0000\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1245 - accuracy: 0.6024 - mse: 0.2241 - precision_3: 0.5796 - recall_2: 1.0000 - val_loss: 1.1527 - val_accuracy: 0.4286 - val_mse: 0.2474 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0911 - accuracy: 0.6386 - mse: 0.2234 - precision_3: 0.6026 - recall_2: 1.0000 - val_loss: 1.1101 - val_accuracy: 0.5714 - val_mse: 0.2414 - val_precision_3: 0.5000 - val_recall_2: 1.0000\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.0601 - accuracy: 0.7229 - mse: 0.2227 - precision_3: 0.6667 - recall_2: 0.9890 - val_loss: 1.0768 - val_accuracy: 0.5238 - val_mse: 0.2391 - val_precision_3: 0.4706 - val_recall_2: 0.8889\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.0267 - accuracy: 0.6627 - mse: 0.2198 - precision_3: 0.6190 - recall_2: 1.0000 - val_loss: 1.0631 - val_accuracy: 0.4286 - val_mse: 0.2452 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0027 - accuracy: 0.6265 - mse: 0.2204 - precision_3: 0.5948 - recall_2: 1.0000 - val_loss: 1.0356 - val_accuracy: 0.5238 - val_mse: 0.2435 - val_precision_3: 0.4737 - val_recall_2: 1.0000\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9788 - accuracy: 0.7229 - mse: 0.2198 - precision_3: 0.6642 - recall_2: 1.0000 - val_loss: 0.9987 - val_accuracy: 0.5714 - val_mse: 0.2360 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9553 - accuracy: 0.7169 - mse: 0.2186 - precision_3: 0.6618 - recall_2: 0.9890 - val_loss: 0.9809 - val_accuracy: 0.5238 - val_mse: 0.2370 - val_precision_3: 0.4706 - val_recall_2: 0.8889\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9346 - accuracy: 0.6747 - mse: 0.2175 - precision_3: 0.6276 - recall_2: 1.0000 - val_loss: 0.9775 - val_accuracy: 0.4286 - val_mse: 0.2436 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.9163 - accuracy: 0.6446 - mse: 0.2165 - precision_3: 0.6067 - recall_2: 1.0000 - val_loss: 0.9540 - val_accuracy: 0.5714 - val_mse: 0.2395 - val_precision_3: 0.5000 - val_recall_2: 1.0000\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.9011 - accuracy: 0.7169 - mse: 0.2159 - precision_3: 0.6642 - recall_2: 0.9780 - val_loss: 0.9270 - val_accuracy: 0.5714 - val_mse: 0.2327 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8832 - accuracy: 0.6988 - mse: 0.2134 - precision_3: 0.6454 - recall_2: 1.0000 - val_loss: 0.9266 - val_accuracy: 0.5714 - val_mse: 0.2385 - val_precision_3: 0.5000 - val_recall_2: 1.0000\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8693 - accuracy: 0.6747 - mse: 0.2121 - precision_3: 0.6276 - recall_2: 1.0000 - val_loss: 0.9112 - val_accuracy: 0.5238 - val_mse: 0.2359 - val_precision_3: 0.4706 - val_recall_2: 0.8889\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8567 - accuracy: 0.6928 - mse: 0.2109 - precision_3: 0.6408 - recall_2: 1.0000 - val_loss: 0.8972 - val_accuracy: 0.5714 - val_mse: 0.2337 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8447 - accuracy: 0.7169 - mse: 0.2094 - precision_3: 0.6594 - recall_2: 1.0000 - val_loss: 0.8855 - val_accuracy: 0.5714 - val_mse: 0.2323 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8344 - accuracy: 0.6807 - mse: 0.2088 - precision_3: 0.6319 - recall_2: 1.0000 - val_loss: 0.8859 - val_accuracy: 0.5714 - val_mse: 0.2363 - val_precision_3: 0.5000 - val_recall_2: 1.0000\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.8255 - accuracy: 0.7169 - mse: 0.2076 - precision_3: 0.6618 - recall_2: 0.9890 - val_loss: 0.8623 - val_accuracy: 0.5714 - val_mse: 0.2280 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.8117 - accuracy: 0.7229 - mse: 0.2044 - precision_3: 0.6642 - recall_2: 1.0000 - val_loss: 0.8668 - val_accuracy: 0.5714 - val_mse: 0.2338 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8044 - accuracy: 0.6867 - mse: 0.2043 - precision_3: 0.6364 - recall_2: 1.0000 - val_loss: 0.8529 - val_accuracy: 0.5714 - val_mse: 0.2297 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7964 - accuracy: 0.7590 - mse: 0.2024 - precision_3: 0.7040 - recall_2: 0.9670 - val_loss: 0.8365 - val_accuracy: 0.6190 - val_mse: 0.2242 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7860 - accuracy: 0.7410 - mse: 0.2001 - precision_3: 0.6818 - recall_2: 0.9890 - val_loss: 0.8497 - val_accuracy: 0.5714 - val_mse: 0.2333 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7781 - accuracy: 0.7470 - mse: 0.1986 - precision_3: 0.6842 - recall_2: 1.0000 - val_loss: 0.8261 - val_accuracy: 0.6190 - val_mse: 0.2239 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7674 - accuracy: 0.7470 - mse: 0.1958 - precision_3: 0.6899 - recall_2: 0.9780 - val_loss: 0.8331 - val_accuracy: 0.5714 - val_mse: 0.2297 - val_precision_3: 0.5000 - val_recall_2: 0.8889\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7623 - accuracy: 0.7349 - mse: 0.1958 - precision_3: 0.6767 - recall_2: 0.9890 - val_loss: 0.8275 - val_accuracy: 0.6190 - val_mse: 0.2290 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7542 - accuracy: 0.7349 - mse: 0.1936 - precision_3: 0.6767 - recall_2: 0.9890 - val_loss: 0.8100 - val_accuracy: 0.6190 - val_mse: 0.2217 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7471 - accuracy: 0.7651 - mse: 0.1911 - precision_3: 0.7203 - recall_2: 0.9341 - val_loss: 0.8071 - val_accuracy: 0.6190 - val_mse: 0.2218 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7402 - accuracy: 0.7711 - mse: 0.1892 - precision_3: 0.7154 - recall_2: 0.9670 - val_loss: 0.8125 - val_accuracy: 0.6190 - val_mse: 0.2257 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7340 - accuracy: 0.7410 - mse: 0.1877 - precision_3: 0.6791 - recall_2: 1.0000 - val_loss: 0.8134 - val_accuracy: 0.6190 - val_mse: 0.2270 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7306 - accuracy: 0.7470 - mse: 0.1866 - precision_3: 0.6899 - recall_2: 0.9780 - val_loss: 0.7973 - val_accuracy: 0.6190 - val_mse: 0.2193 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7232 - accuracy: 0.7711 - mse: 0.1831 - precision_3: 0.7227 - recall_2: 0.9451 - val_loss: 0.8005 - val_accuracy: 0.6190 - val_mse: 0.2216 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7204 - accuracy: 0.7651 - mse: 0.1825 - precision_3: 0.7167 - recall_2: 0.9451 - val_loss: 0.8015 - val_accuracy: 0.6190 - val_mse: 0.2227 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7157 - accuracy: 0.7711 - mse: 0.1807 - precision_3: 0.7265 - recall_2: 0.9341 - val_loss: 0.7922 - val_accuracy: 0.6190 - val_mse: 0.2187 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7125 - accuracy: 0.7711 - mse: 0.1802 - precision_3: 0.7087 - recall_2: 0.9890 - val_loss: 0.7896 - val_accuracy: 0.6190 - val_mse: 0.2177 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7068 - accuracy: 0.7711 - mse: 0.1771 - precision_3: 0.7387 - recall_2: 0.9011 - val_loss: 0.7807 - val_accuracy: 0.6190 - val_mse: 0.2140 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.7009 - accuracy: 0.7590 - mse: 0.1753 - precision_3: 0.7107 - recall_2: 0.9451 - val_loss: 0.7984 - val_accuracy: 0.6190 - val_mse: 0.2230 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6980 - accuracy: 0.7892 - mse: 0.1741 - precision_3: 0.7333 - recall_2: 0.9670 - val_loss: 0.7689 - val_accuracy: 0.6667 - val_mse: 0.2087 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.7952 - mse: 0.1709 - precision_3: 0.7615 - recall_2: 0.9121 - val_loss: 0.7762 - val_accuracy: 0.6190 - val_mse: 0.2125 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6892 - accuracy: 0.7711 - mse: 0.1700 - precision_3: 0.7227 - recall_2: 0.9451 - val_loss: 0.7672 - val_accuracy: 0.6667 - val_mse: 0.2081 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6835 - accuracy: 0.8012 - mse: 0.1668 - precision_3: 0.7843 - recall_2: 0.8791 - val_loss: 0.7688 - val_accuracy: 0.6667 - val_mse: 0.2090 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6808 - accuracy: 0.7771 - mse: 0.1665 - precision_3: 0.7250 - recall_2: 0.9560 - val_loss: 0.7739 - val_accuracy: 0.6190 - val_mse: 0.2118 - val_precision_3: 0.5333 - val_recall_2: 0.8889\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.7952 - mse: 0.1682 - precision_3: 0.7714 - recall_2: 0.8901 - val_loss: 0.7528 - val_accuracy: 0.6667 - val_mse: 0.2015 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.7831 - mse: 0.1667 - precision_3: 0.7311 - recall_2: 0.9560 - val_loss: 0.7627 - val_accuracy: 0.6667 - val_mse: 0.2064 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6690 - accuracy: 0.8253 - mse: 0.1605 - precision_3: 0.8163 - recall_2: 0.8791 - val_loss: 0.7453 - val_accuracy: 0.7143 - val_mse: 0.1984 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6652 - accuracy: 0.8012 - mse: 0.1597 - precision_3: 0.7636 - recall_2: 0.9231 - val_loss: 0.7580 - val_accuracy: 0.6667 - val_mse: 0.2049 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.8012 - mse: 0.1579 - precision_3: 0.7736 - recall_2: 0.9011 - val_loss: 0.7432 - val_accuracy: 0.6667 - val_mse: 0.1980 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6551 - accuracy: 0.8133 - mse: 0.1551 - precision_3: 0.7941 - recall_2: 0.8901 - val_loss: 0.7481 - val_accuracy: 0.6667 - val_mse: 0.2007 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6564 - accuracy: 0.7831 - mse: 0.1565 - precision_3: 0.7434 - recall_2: 0.9231 - val_loss: 0.7419 - val_accuracy: 0.6667 - val_mse: 0.1979 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6501 - accuracy: 0.8253 - mse: 0.1529 - precision_3: 0.7925 - recall_2: 0.9231 - val_loss: 0.7380 - val_accuracy: 0.6667 - val_mse: 0.1956 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6596 - accuracy: 0.8373 - mse: 0.1570 - precision_3: 0.8636 - recall_2: 0.8352 - val_loss: 0.7350 - val_accuracy: 0.6667 - val_mse: 0.1943 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6415 - accuracy: 0.8193 - mse: 0.1488 - precision_3: 0.7905 - recall_2: 0.9121 - val_loss: 0.7554 - val_accuracy: 0.6667 - val_mse: 0.2034 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6400 - accuracy: 0.7952 - mse: 0.1485 - precision_3: 0.7615 - recall_2: 0.9121 - val_loss: 0.7393 - val_accuracy: 0.6667 - val_mse: 0.1963 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6359 - accuracy: 0.8133 - mse: 0.1462 - precision_3: 0.8061 - recall_2: 0.8681 - val_loss: 0.7325 - val_accuracy: 0.7143 - val_mse: 0.1934 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6389 - accuracy: 0.8193 - mse: 0.1475 - precision_3: 0.8081 - recall_2: 0.8791 - val_loss: 0.7547 - val_accuracy: 0.6667 - val_mse: 0.2022 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6300 - accuracy: 0.8012 - mse: 0.1434 - precision_3: 0.7843 - recall_2: 0.8791 - val_loss: 0.7287 - val_accuracy: 0.7143 - val_mse: 0.1914 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6313 - accuracy: 0.8133 - mse: 0.1443 - precision_3: 0.8000 - recall_2: 0.8791 - val_loss: 0.7271 - val_accuracy: 0.7143 - val_mse: 0.1906 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6265 - accuracy: 0.8313 - mse: 0.1418 - precision_3: 0.8247 - recall_2: 0.8791 - val_loss: 0.7320 - val_accuracy: 0.7143 - val_mse: 0.1927 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6244 - accuracy: 0.8313 - mse: 0.1413 - precision_3: 0.8316 - recall_2: 0.8681 - val_loss: 0.7431 - val_accuracy: 0.6667 - val_mse: 0.1968 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6227 - accuracy: 0.8133 - mse: 0.1407 - precision_3: 0.7830 - recall_2: 0.9121 - val_loss: 0.7364 - val_accuracy: 0.6667 - val_mse: 0.1939 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6175 - accuracy: 0.8193 - mse: 0.1383 - precision_3: 0.8144 - recall_2: 0.8681 - val_loss: 0.7367 - val_accuracy: 0.7143 - val_mse: 0.1940 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6177 - accuracy: 0.8133 - mse: 0.1385 - precision_3: 0.8125 - recall_2: 0.8571 - val_loss: 0.7318 - val_accuracy: 0.7143 - val_mse: 0.1921 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6125 - accuracy: 0.8313 - mse: 0.1363 - precision_3: 0.8247 - recall_2: 0.8791 - val_loss: 0.7364 - val_accuracy: 0.7143 - val_mse: 0.1938 - val_precision_3: 0.6154 - val_recall_2: 0.8889\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6167 - accuracy: 0.8253 - mse: 0.1383 - precision_3: 0.8039 - recall_2: 0.9011 - val_loss: 0.7242 - val_accuracy: 0.7619 - val_mse: 0.1888 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6185 - accuracy: 0.8072 - mse: 0.1382 - precision_3: 0.7706 - recall_2: 0.9231 - val_loss: 0.7534 - val_accuracy: 0.6667 - val_mse: 0.1992 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6219 - accuracy: 0.8434 - mse: 0.1407 - precision_3: 0.8736 - recall_2: 0.8352 - val_loss: 0.7118 - val_accuracy: 0.7619 - val_mse: 0.1834 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6192 - accuracy: 0.8313 - mse: 0.1383 - precision_3: 0.7944 - recall_2: 0.9341 - val_loss: 0.7389 - val_accuracy: 0.6667 - val_mse: 0.1939 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6012 - accuracy: 0.8373 - mse: 0.1311 - precision_3: 0.8265 - recall_2: 0.8901 - val_loss: 0.7107 - val_accuracy: 0.7619 - val_mse: 0.1830 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6124 - accuracy: 0.8614 - mse: 0.1371 - precision_3: 0.8864 - recall_2: 0.8571 - val_loss: 0.7366 - val_accuracy: 0.6667 - val_mse: 0.1926 - val_precision_3: 0.5833 - val_recall_2: 0.7778\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6014 - accuracy: 0.8193 - mse: 0.1312 - precision_3: 0.7961 - recall_2: 0.9011 - val_loss: 0.7261 - val_accuracy: 0.7619 - val_mse: 0.1888 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6032 - accuracy: 0.8373 - mse: 0.1328 - precision_3: 0.8404 - recall_2: 0.8681 - val_loss: 0.7390 - val_accuracy: 0.6667 - val_mse: 0.1935 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5979 - accuracy: 0.8193 - mse: 0.1299 - precision_3: 0.7961 - recall_2: 0.9011 - val_loss: 0.7319 - val_accuracy: 0.7143 - val_mse: 0.1908 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5959 - accuracy: 0.8253 - mse: 0.1291 - precision_3: 0.8100 - recall_2: 0.8901 - val_loss: 0.7285 - val_accuracy: 0.7143 - val_mse: 0.1896 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5928 - accuracy: 0.8253 - mse: 0.1277 - precision_3: 0.8163 - recall_2: 0.8791 - val_loss: 0.7127 - val_accuracy: 0.7619 - val_mse: 0.1839 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5956 - accuracy: 0.8253 - mse: 0.1286 - precision_3: 0.8100 - recall_2: 0.8901 - val_loss: 0.7139 - val_accuracy: 0.7619 - val_mse: 0.1843 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5914 - accuracy: 0.8253 - mse: 0.1273 - precision_3: 0.8298 - recall_2: 0.8571 - val_loss: 0.7155 - val_accuracy: 0.7619 - val_mse: 0.1851 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5937 - accuracy: 0.8373 - mse: 0.1285 - precision_3: 0.8478 - recall_2: 0.8571 - val_loss: 0.7164 - val_accuracy: 0.7619 - val_mse: 0.1853 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6023 - accuracy: 0.8373 - mse: 0.1315 - precision_3: 0.7909 - recall_2: 0.9560 - val_loss: 0.7323 - val_accuracy: 0.7619 - val_mse: 0.1904 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6198 - accuracy: 0.7831 - mse: 0.1413 - precision_3: 0.8235 - recall_2: 0.7692 - val_loss: 0.7105 - val_accuracy: 0.7619 - val_mse: 0.1828 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6017 - accuracy: 0.8133 - mse: 0.1316 - precision_3: 0.7632 - recall_2: 0.9560 - val_loss: 0.7233 - val_accuracy: 0.7619 - val_mse: 0.1874 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5986 - accuracy: 0.8313 - mse: 0.1319 - precision_3: 0.8539 - recall_2: 0.8352 - val_loss: 0.7185 - val_accuracy: 0.7619 - val_mse: 0.1858 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.5807 - accuracy: 0.8253 - mse: 0.1230 - precision_3: 0.8100 - recall_2: 0.8901 - val_loss: 0.7423 - val_accuracy: 0.6667 - val_mse: 0.1933 - val_precision_3: 0.5714 - val_recall_2: 0.8889\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5835 - accuracy: 0.8313 - mse: 0.1247 - precision_3: 0.8119 - recall_2: 0.9011 - val_loss: 0.7191 - val_accuracy: 0.7619 - val_mse: 0.1858 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5873 - accuracy: 0.8253 - mse: 0.1258 - precision_3: 0.7925 - recall_2: 0.9231 - val_loss: 0.7218 - val_accuracy: 0.7143 - val_mse: 0.1864 - val_precision_3: 0.6364 - val_recall_2: 0.7778\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.5861 - accuracy: 0.8193 - mse: 0.1253 - precision_3: 0.8081 - recall_2: 0.8791 - val_loss: 0.7090 - val_accuracy: 0.7619 - val_mse: 0.1822 - val_precision_3: 0.7000 - val_recall_2: 0.7778\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5894 - accuracy: 0.8494 - mse: 0.1276 - precision_3: 0.8587 - recall_2: 0.8681 - val_loss: 0.7335 - val_accuracy: 0.7619 - val_mse: 0.1898 - val_precision_3: 0.6667 - val_recall_2: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model4\\assets\n"
     ]
    }
   ],
   "source": [
    "fitModel(tf_model4, 'model4', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "d0185bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 1s 36ms/step - loss: 0.7094 - accuracy: 0.4699 - mse: 0.2578 - precision_3: 0.5361 - recall_2: 0.5200 - val_loss: 0.6974 - val_accuracy: 0.3810 - val_mse: 0.2521 - val_precision_3: 0.4000 - val_recall_2: 0.8889\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6893 - accuracy: 0.5361 - mse: 0.2481 - precision_3: 0.5673 - recall_2: 0.6484 - val_loss: 0.6978 - val_accuracy: 0.4286 - val_mse: 0.2523 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6864 - accuracy: 0.5964 - mse: 0.2467 - precision_3: 0.6034 - recall_2: 0.7692 - val_loss: 0.6982 - val_accuracy: 0.4286 - val_mse: 0.2525 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.4940 - mse: 0.2498 - precision_3: 0.5276 - recall_2: 0.7363 - val_loss: 0.6987 - val_accuracy: 0.4286 - val_mse: 0.2528 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6956 - accuracy: 0.5241 - mse: 0.2512 - precision_3: 0.5526 - recall_2: 0.6923 - val_loss: 0.6980 - val_accuracy: 0.4286 - val_mse: 0.2524 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6900 - accuracy: 0.5542 - mse: 0.2484 - precision_3: 0.5659 - recall_2: 0.8022 - val_loss: 0.6982 - val_accuracy: 0.4286 - val_mse: 0.2525 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6843 - accuracy: 0.5120 - mse: 0.2456 - precision_3: 0.5373 - recall_2: 0.7912 - val_loss: 0.6984 - val_accuracy: 0.4286 - val_mse: 0.2526 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6986 - accuracy: 0.5602 - mse: 0.2523 - precision_3: 0.5608 - recall_2: 0.9121 - val_loss: 0.7002 - val_accuracy: 0.4286 - val_mse: 0.2536 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6929 - accuracy: 0.5542 - mse: 0.2496 - precision_3: 0.5578 - recall_2: 0.9011 - val_loss: 0.6999 - val_accuracy: 0.4286 - val_mse: 0.2534 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6725 - accuracy: 0.5602 - mse: 0.2399 - precision_3: 0.5556 - recall_2: 0.9890 - val_loss: 0.7015 - val_accuracy: 0.4286 - val_mse: 0.2542 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5000 - mse: 0.2467 - precision_3: 0.5270 - recall_2: 0.8571 - val_loss: 0.7002 - val_accuracy: 0.4286 - val_mse: 0.2535 - val_precision_3: 0.4286 - val_recall_2: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model5\\assets\n"
     ]
    }
   ],
   "source": [
    "fitModel(tf_model5, 'model5', [earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "2085f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pomocna funkcia\n",
    "def predictEval(tf_model, XX, yy):\n",
    "    # vykonanie predikcie\n",
    "    y_pred = tf_model.predict(XX)\n",
    "    # uprava outputu na boolean\n",
    "    y_pred_bool = np.copy(y_pred)\n",
    "    for x in y_pred_bool:\n",
    "        x[0] = round(x[0])\n",
    "    y_pred_bool\n",
    "\n",
    "    #vratenie y a accuaracy\n",
    "    return [y_pred, y_pred_bool, accuracy_score(y_pred_bool, yy), mean_squared_error(y_pred_bool, yy), precision_score(y_pred_bool, yy), recall_score(y_pred_bool, yy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ac23f",
   "metadata": {},
   "source": [
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "56957029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictEvalWrap(model, name):\n",
    "    train = predictEval(model, X_train, y_train)\n",
    "    val = predictEval(model, X_val, y_val)\n",
    "    test = predictEval(model, X_test, y_test)\n",
    "\n",
    "    print(name)\n",
    "    print('Accuracy score')\n",
    "    print(f'Train: {train[2]*100:.2f}%')\n",
    "    print(f'Val: {val[2]*100:.2f}%')\n",
    "    print(f'Test: {test[2]*100:.2f}%')\n",
    "    print('Mean squared error')\n",
    "    print(f'Train: {train[3]*100:.2f}%')\n",
    "    print(f'Val: {val[3]*100:.2f}%')\n",
    "    print(f'Test: {test[3]*100:.2f}%')\n",
    "    print('Precision')\n",
    "    print(f'Train: {train[4]*100:.2f}%')\n",
    "    print(f'Val: {val[4]*100:.2f}%')\n",
    "    print(f'Test: {test[4]*100:.2f}%')\n",
    "    print('Recall')\n",
    "    print(f'Train: {train[5]*100:.2f}%')\n",
    "    print(f'Val: {val[5]*100:.2f}%')\n",
    "    print(f'Test: {test[5]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "b444aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1000us/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "No overfit prevention\n",
      "Accuracy score\n",
      "Train: 96.99%\n",
      "Val: 71.43%\n",
      "Test: 76.19%\n",
      "Mean squared error\n",
      "Train: 3.01%\n",
      "Val: 28.57%\n",
      "Test: 23.81%\n",
      "Precision\n",
      "Train: 97.80%\n",
      "Val: 77.78%\n",
      "Test: 90.91%\n",
      "Recall\n",
      "Train: 96.74%\n",
      "Val: 63.64%\n",
      "Test: 71.43%\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Dropout\n",
      "Accuracy score\n",
      "Train: 90.96%\n",
      "Val: 71.43%\n",
      "Test: 76.19%\n",
      "Mean squared error\n",
      "Train: 9.04%\n",
      "Val: 28.57%\n",
      "Test: 23.81%\n",
      "Precision\n",
      "Train: 94.51%\n",
      "Val: 77.78%\n",
      "Test: 81.82%\n",
      "Recall\n",
      "Train: 89.58%\n",
      "Val: 63.64%\n",
      "Test: 75.00%\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Early stopping\n",
      "Accuracy score\n",
      "Train: 89.16%\n",
      "Val: 66.67%\n",
      "Test: 76.19%\n",
      "Mean squared error\n",
      "Train: 10.84%\n",
      "Val: 33.33%\n",
      "Test: 23.81%\n",
      "Precision\n",
      "Train: 93.41%\n",
      "Val: 77.78%\n",
      "Test: 90.91%\n",
      "Recall\n",
      "Train: 87.63%\n",
      "Val: 58.33%\n",
      "Test: 71.43%\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Regularization\n",
      "Accuracy score\n",
      "Train: 83.13%\n",
      "Val: 76.19%\n",
      "Test: 76.19%\n",
      "Mean squared error\n",
      "Train: 16.87%\n",
      "Val: 23.81%\n",
      "Test: 23.81%\n",
      "Precision\n",
      "Train: 90.11%\n",
      "Val: 88.89%\n",
      "Test: 81.82%\n",
      "Recall\n",
      "Train: 81.19%\n",
      "Val: 66.67%\n",
      "Test: 75.00%\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Dropout + Early stopping\n",
      "Accuracy score\n",
      "Train: 54.82%\n",
      "Val: 42.86%\n",
      "Test: 52.38%\n",
      "Mean squared error\n",
      "Train: 45.18%\n",
      "Val: 57.14%\n",
      "Test: 47.62%\n",
      "Precision\n",
      "Train: 100.00%\n",
      "Val: 100.00%\n",
      "Test: 100.00%\n",
      "Recall\n",
      "Train: 54.82%\n",
      "Val: 42.86%\n",
      "Test: 52.38%\n"
     ]
    }
   ],
   "source": [
    "predictEvalWrap(tf_model1, 'No overfit prevention')\n",
    "predictEvalWrap(tf_model2, 'Dropout')\n",
    "predictEvalWrap(tf_model3, 'Early stopping')\n",
    "predictEvalWrap(tf_model4, 'Regularization')\n",
    "predictEvalWrap(tf_model5, 'Dropout + Early stopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "2a688b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f17a7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch model\n",
    "class PyTorchModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PyTorchModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5d7e8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class SonarDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "65da1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SonarDataset(X_train, y_train)\n",
    "val_dataset = SonarDataset(X_val, y_val)\n",
    "test_dataset = SonarDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "06616cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_dim = len(X_train.columns)\n",
    "model = PyTorchModel(input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "e1b6f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "e9157ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Loss: 0.6958, Val Loss: 0.6840\n",
      "Epoch: 2/100, Loss: 0.6921, Val Loss: 0.6839\n",
      "Epoch: 3/100, Loss: 0.6750, Val Loss: 0.6840\n",
      "Epoch: 4/100, Loss: 0.7103, Val Loss: 0.6843\n",
      "Epoch: 5/100, Loss: 0.6972, Val Loss: 0.6853\n",
      "Epoch: 6/100, Loss: 0.6907, Val Loss: 0.6855\n",
      "Epoch: 7/100, Loss: 0.6837, Val Loss: 0.6837\n",
      "Epoch: 8/100, Loss: 0.6572, Val Loss: 0.6824\n",
      "Epoch: 9/100, Loss: 0.6793, Val Loss: 0.6812\n",
      "Epoch: 10/100, Loss: 0.6260, Val Loss: 0.6724\n",
      "Epoch: 11/100, Loss: 0.7068, Val Loss: 0.6684\n",
      "Epoch: 12/100, Loss: 0.7018, Val Loss: 0.6511\n",
      "Epoch: 13/100, Loss: 0.5623, Val Loss: 0.6531\n",
      "Epoch: 14/100, Loss: 0.5361, Val Loss: 0.6387\n",
      "Epoch: 15/100, Loss: 0.6531, Val Loss: 0.6265\n",
      "Epoch: 16/100, Loss: 0.7095, Val Loss: 0.6199\n",
      "Epoch: 17/100, Loss: 0.6362, Val Loss: 0.6144\n",
      "Epoch: 18/100, Loss: 0.4254, Val Loss: 0.5930\n",
      "Epoch: 19/100, Loss: 0.3753, Val Loss: 0.6083\n",
      "Epoch: 20/100, Loss: 0.4039, Val Loss: 0.5877\n",
      "Epoch: 21/100, Loss: 0.4912, Val Loss: 0.5943\n",
      "Epoch: 22/100, Loss: 0.5207, Val Loss: 0.5789\n",
      "Epoch: 23/100, Loss: 0.3568, Val Loss: 0.5723\n",
      "Epoch: 24/100, Loss: 0.2852, Val Loss: 0.5834\n",
      "Epoch: 25/100, Loss: 0.4712, Val Loss: 0.5732\n",
      "Epoch: 26/100, Loss: 0.2267, Val Loss: 0.5955\n",
      "Epoch: 27/100, Loss: 0.3083, Val Loss: 0.5666\n",
      "Epoch: 28/100, Loss: 0.5157, Val Loss: 0.5938\n",
      "Epoch: 29/100, Loss: 0.3984, Val Loss: 0.5788\n",
      "Epoch: 30/100, Loss: 0.2906, Val Loss: 0.5955\n",
      "Epoch: 31/100, Loss: 0.5207, Val Loss: 0.5808\n",
      "Epoch: 32/100, Loss: 0.1365, Val Loss: 0.5977\n",
      "Epoch: 33/100, Loss: 0.3826, Val Loss: 0.6028\n",
      "Epoch: 34/100, Loss: 0.1795, Val Loss: 0.5905\n",
      "Epoch: 35/100, Loss: 0.9741, Val Loss: 0.6112\n",
      "Epoch: 36/100, Loss: 0.3084, Val Loss: 0.6141\n",
      "Epoch: 37/100, Loss: 0.3582, Val Loss: 0.5892\n",
      "Epoch: 38/100, Loss: 0.7020, Val Loss: 0.6047\n",
      "Epoch: 39/100, Loss: 0.4154, Val Loss: 0.6068\n",
      "Epoch: 40/100, Loss: 0.8280, Val Loss: 0.6022\n",
      "Epoch: 41/100, Loss: 0.5818, Val Loss: 0.6082\n",
      "Epoch: 42/100, Loss: 0.3496, Val Loss: 0.6169\n",
      "Epoch: 43/100, Loss: 0.2750, Val Loss: 0.5915\n",
      "Epoch: 44/100, Loss: 0.3335, Val Loss: 0.6129\n",
      "Epoch: 45/100, Loss: 0.7613, Val Loss: 0.6039\n",
      "Epoch: 46/100, Loss: 0.1288, Val Loss: 0.6237\n",
      "Epoch: 47/100, Loss: 0.3369, Val Loss: 0.6159\n",
      "Epoch: 48/100, Loss: 0.4341, Val Loss: 0.6241\n",
      "Epoch: 49/100, Loss: 0.6008, Val Loss: 0.6196\n",
      "Epoch: 50/100, Loss: 0.7117, Val Loss: 0.6268\n",
      "Epoch: 51/100, Loss: 0.2154, Val Loss: 0.6188\n",
      "Epoch: 52/100, Loss: 0.4532, Val Loss: 0.6345\n",
      "Epoch: 53/100, Loss: 0.3713, Val Loss: 0.6286\n",
      "Epoch: 54/100, Loss: 0.2558, Val Loss: 0.6278\n",
      "Epoch: 55/100, Loss: 1.0519, Val Loss: 0.6423\n",
      "Epoch: 56/100, Loss: 0.3296, Val Loss: 0.6318\n",
      "Epoch: 57/100, Loss: 0.1140, Val Loss: 0.6435\n",
      "Epoch: 58/100, Loss: 0.1283, Val Loss: 0.6466\n",
      "Epoch: 59/100, Loss: 0.5570, Val Loss: 0.6457\n",
      "Epoch: 60/100, Loss: 0.7003, Val Loss: 0.6701\n",
      "Epoch: 61/100, Loss: 0.2336, Val Loss: 0.6482\n",
      "Epoch: 62/100, Loss: 0.2049, Val Loss: 0.6459\n",
      "Epoch: 63/100, Loss: 0.3766, Val Loss: 0.6580\n",
      "Epoch: 64/100, Loss: 0.1430, Val Loss: 0.6487\n",
      "Epoch: 65/100, Loss: 0.2498, Val Loss: 0.6622\n",
      "Epoch: 66/100, Loss: 0.1249, Val Loss: 0.6593\n",
      "Epoch: 67/100, Loss: 0.6503, Val Loss: 0.6638\n",
      "Epoch: 68/100, Loss: 0.8422, Val Loss: 0.6683\n",
      "Epoch: 69/100, Loss: 0.2435, Val Loss: 0.6528\n",
      "Epoch: 70/100, Loss: 0.0345, Val Loss: 0.6720\n",
      "Epoch: 71/100, Loss: 0.8233, Val Loss: 0.6639\n",
      "Epoch: 72/100, Loss: 0.2522, Val Loss: 0.6640\n",
      "Epoch: 73/100, Loss: 0.0809, Val Loss: 0.6808\n",
      "Epoch: 74/100, Loss: 0.0937, Val Loss: 0.6732\n",
      "Epoch: 75/100, Loss: 0.3027, Val Loss: 0.6750\n",
      "Epoch: 76/100, Loss: 0.1477, Val Loss: 0.6732\n",
      "Epoch: 77/100, Loss: 0.5603, Val Loss: 0.6940\n",
      "Epoch: 78/100, Loss: 0.2188, Val Loss: 0.6891\n",
      "Epoch: 79/100, Loss: 0.0747, Val Loss: 0.7050\n",
      "Epoch: 80/100, Loss: 0.1851, Val Loss: 0.6933\n",
      "Epoch: 81/100, Loss: 0.1556, Val Loss: 0.7047\n",
      "Epoch: 82/100, Loss: 0.2064, Val Loss: 0.7019\n",
      "Epoch: 83/100, Loss: 0.2173, Val Loss: 0.7272\n",
      "Epoch: 84/100, Loss: 0.2225, Val Loss: 0.7139\n",
      "Epoch: 85/100, Loss: 0.1434, Val Loss: 0.7286\n",
      "Epoch: 86/100, Loss: 0.1637, Val Loss: 0.7296\n",
      "Epoch: 87/100, Loss: 0.1740, Val Loss: 0.7374\n",
      "Epoch: 88/100, Loss: 0.3105, Val Loss: 0.7495\n",
      "Epoch: 89/100, Loss: 0.3540, Val Loss: 0.7476\n",
      "Epoch: 90/100, Loss: 0.1851, Val Loss: 0.7538\n",
      "Epoch: 91/100, Loss: 0.0774, Val Loss: 0.7611\n",
      "Epoch: 92/100, Loss: 0.2391, Val Loss: 0.7627\n",
      "Epoch: 93/100, Loss: 0.0418, Val Loss: 0.7798\n",
      "Epoch: 94/100, Loss: 0.1251, Val Loss: 0.7728\n",
      "Epoch: 95/100, Loss: 0.1220, Val Loss: 0.7827\n",
      "Epoch: 96/100, Loss: 0.0435, Val Loss: 0.7683\n",
      "Epoch: 97/100, Loss: 0.1333, Val Loss: 0.7885\n",
      "Epoch: 98/100, Loss: 0.4567, Val Loss: 0.7796\n",
      "Epoch: 99/100, Loss: 0.1947, Val Loss: 0.7886\n",
      "Epoch: 100/100, Loss: 0.6207, Val Loss: 0.8014\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "1ab49a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_eval(model, loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            y_pred_bool = torch.round(y_pred)\n",
    "            accuracy = accuracy_score(y_pred_bool, y_batch)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "ce38c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = predict_eval(model, train_loader)\n",
    "val_accuracy = predict_eval(model, val_loader)\n",
    "test_accuracy = predict_eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "5df66fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score\n",
      "Train: 50.00%\n",
      "Val: 71.43%\n",
      "Test: 76.19%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score')\n",
    "print(f'Train: {train_accuracy * 100:.2f}%')\n",
    "print(f'Val: {val_accuracy * 100:.2f}%')\n",
    "print(f'Test: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db5e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
